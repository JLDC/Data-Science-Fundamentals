{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a96a64-b14a-4947-ba69-abdc0096177c",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/106_ols-train-test-cv.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b2767-dcc7-4292-9caa-e4049bcaaf91",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS), Overfitting, Training / Testing, and Cross-Validation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc1fb8-3e88-4913-a9ea-ecde1830d820",
   "metadata": {},
   "source": [
    "In this notebook we will cover several new and important concepts of machine learning simultaneously. As you will soon discover, we have (purposefully) ignored **some crucial concepts** so far, as we have focused on the engineering of the learning machine. But there is clearly more to machine learning! \n",
    "\n",
    "\n",
    "### üßë‚Äçüíª <font color=green>**Your Task**</font>\n",
    "\n",
    "Read the notebook sections, try to understand the code. Most importantly, understand the pictures and answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf7c40-5830-4a74-934e-f5db2e3b51c8",
   "metadata": {},
   "source": [
    "___\n",
    "## Ordinary Least Squares (OLS)\n",
    "In notebook 05, you have already become accustomed to the most standard method of regression: ordinary least squares (OLS). Recall that the OLS model consists of building a linear relationship between a target variable $\\mathbf{y}$ and some features $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_k$. \n",
    "___\n",
    "### Mathematical intuition\n",
    "One of most important (if not the most important) assumption of OLS is that the target variable ${y}$ is related to the feature variables $x_1, x_2, \\dots, x_p$ in a linear manner (at least for the purpose of making reasonably good predictions):\n",
    "\n",
    "$${y} = b + w_1 \\cdot {x}_1 + w_2 \\cdot {x}_2 + \\dots + w_p \\cdot {x}_p + {\\epsilon},$$\n",
    "\n",
    "where ${\\epsilon}$ is some randomly distributed error term with zero mean.\n",
    "\n",
    "Our main goal is to find an estimate of weights $b, w_1, w_2, \\dots, w_p$, which we will denote by $\\hat{b}, \\hat{w}_1, \\hat{w}_2, \\dots, \\hat{w}_k$. Once we have this estimate, it is easy to build a prediction:\n",
    "\n",
    "$$\\hat{{y}} = \\hat{b} + \\hat{w}_1 \\cdot {x}_1 + \\hat{w}_2 \\cdot {x}_2 + \\dots + \\hat{w}_p \\cdot {x}_p$$\n",
    "\n",
    "However, regression folks do not like so much that notation, they like greek letters. So they would write:\n",
    "\n",
    "$${y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_p \\cdot x_p + \\epsilon$$\n",
    "\n",
    "In some books, you will also find an $\\alpha$ instead of $\\beta_0$. OLS people normally call the weights *coefficients*. This simply means that they are used to multiply the features. \n",
    "\n",
    "It is also common to use vector and matrix notation. Let $\\mathbf{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p]^\\top$ denote the $(p+1) \\times 1$ vector of weights, and let $\\mathbf x_i = [1, x_{1i}, x_{2i}, \\dots, x_{pi}]^\\top$ denote the $(p+1) \\times 1$ vector of feature values of case $i$. Then we can can also write \n",
    "\n",
    "$$\n",
    "y_i = \\mathbf \\beta^\\top \\mathbf x_i\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{\\hat \\beta}^\\top \\mathbf x_i\n",
    "$$\n",
    "\n",
    "___\n",
    "#### üôÄ ü§Ø Closed-form solution vs. gradient descent (you can skip this in a first reading)\n",
    "We have viewed and discussed the use of gradient descent to find a minimizing solution for the MSE in the preceding notebooks. However, when estimating OLS, we do not actually need gradient descent. Why is that?\n",
    "\n",
    "Well, as it turns out, there exists a **closed-form solution** to OLS which minimizes the mean squared error. You can think of a closed-form expression as a formula, i.e., it doesn't require running multiple steps like an algorithm but you can simply plug the variables in an equation to obtain the result. So, while we could theoretically use gradient descent to find the weights, there exists a formula which will generally be faster.\n",
    "\n",
    "Define $\\mathbf{X} = [\\mathbf{x}^\\top_1 \\, \\mathbf{x}^\\top_2 \\, \\dots \\, \\mathbf{x}^\\top_N]$ as the $N \\times (p+1)$ matrix of features (with a first column of only ones); this is also called the \"design matrix\". Further, define $\\mathbf y = [y_i, y_2, \\dots, y_N]^\\top$ the $N \\times 1$ vector of target value for all cases. The OLS estimate is then given by the matrix equation\n",
    "\n",
    "$$\\hat{\\mathbf{\\beta}} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "On another note, when estimating machine learning models that do use gradient descent, we never explicitly write the gradient descent algorithm ourselves, there are library which have been written in a very efficient manner that allow us to focus on other aspects of the problem. Why did we nevertheless do it in this course? For you to et a glimpse of what happens under the hood of a machine learnign algorithm!\n",
    "\n",
    "One more element of jargon. Regression people also like to call the **target variable** an **independent variable** and the **feature variables** are also called **dependent variables**. This language makes more sense in classical statistics or econometrics where we link \"exogeneous\" independent variables to an \"endogenous\" dependent variable, and the link is often taken as causal. For you, it's just important to know this terminology, so you can talk to statisticians :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18506647-fe20-4149-befd-3d45e67693a0",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing\n",
    "\n",
    "That's enough math for now. Let's have some fun and turn to coding!\n",
    "\n",
    "We will work with an agricultural research dataset on U.S. crop yields consisting of only two columns: \n",
    "1. the temperature `temp`\n",
    "2. the crop yield `yield`\n",
    "\n",
    "For more information view: https://www.pnas.org/content/106/37/15594, or search the web for keywords *temperature*, and *crop yield*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85140431-0a12-4fbc-9220-47294e130ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np # Numerical computation package\n",
    "import pandas as pd # Dataframe package\n",
    "import matplotlib.pyplot as plt # Plotting package\n",
    "# Machine learning objects\n",
    "from sklearn.linear_model import LinearRegression # OLS\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a3980-5c5d-4300-8b84-5f5df801c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the crop yield dataset\n",
    "crops = pd.read_csv(f\"{DATA_PATH}/data/us_crops.csv\")\n",
    "# Sort the data by temperature, \n",
    "# this is not needed for the estimation but will help with the plotting later\n",
    "crops.sort_values(\"temp\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278edb9a-b07a-4ffc-bb58-e82547d2e64b",
   "metadata": {},
   "source": [
    "Let's start with a short visual inspection of the data using a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590545f-1561-4d74-88da-adfa923b8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Add scatterplot\n",
    "ax.scatter(crops[\"temp\"], crops[\"yield\"])\n",
    "# Label axes\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Yield\")\n",
    "# Add a grid\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad4e44-3cb3-48a5-bcd0-02b556eaedaf",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "\n",
    "Can you spot any patterns in the data? Can you explain the patterns you spot? Discuss with your classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff4e56-cf2b-4340-96ef-b18f4d358c1a",
   "metadata": {},
   "source": [
    "___\n",
    "## Baseline regression with full dataset\n",
    "\n",
    "As you already know, running a linear regression in Python using `scikit-learn` (to which `sklearn` belongs to) is an easy task. Let us define the `yield` as our target variable, and the `temp` as our feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78f726-3e06-4e93-9567-41f84d646868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our features and labels (‚ö†Ô∏è notice the double bracket for features ‚ö†Ô∏è)\n",
    "X, y = crops[[\"temp\"]], crops[\"yield\"] \n",
    "# Define the estimator\n",
    "ols1 = LinearRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43a121-54c8-449a-b3e1-6cf6eed0d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the estimator \n",
    "ols1.fit(X, y)\n",
    "# Add the predictions to our `crops` dataframe\n",
    "crops[\"pred\"] = ols1.predict(X)\n",
    "\n",
    "# Get value of constant and coefficient\n",
    "print(f\"constant: {ols1.intercept_:.2f}, coefficient: {ols1.coef_[0]:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07e9c9-21f9-4d04-a5c4-c61a33359273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our predictions\n",
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Add scatterplot\n",
    "ax.scatter(crops[\"temp\"], crops[\"yield\"], label=\"Data\")\n",
    "ax.plot(crops[\"temp\"], crops[\"pred\"], \"-o\", label=\"Prediction (1)\", color=\"orange\")\n",
    "# Label axes\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Yield\")\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "# Add a legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bd8a4-9363-45bd-9962-c35accebe7a5",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "Notice how our predictions follow a straight line. Is this what you would have expected? Why? Why not? Think about the coefficients estimated by OLS, what are they?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4856e15-2fff-454d-9f68-a0c7982e6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE of our model\n",
    "mse = np.mean((crops[\"pred\"] - crops[\"yield\"]) ** 2)\n",
    "mse # Display the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0c8c2-7763-49d3-b2a9-f86848692ffd",
   "metadata": {},
   "source": [
    "The MSE can sometimes be hard to interpret due to the squares, but we can also look at other measures, e.g., the **mean absolute error (MAE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4137ac2-6d1b-458e-88ef-a83e0b7b3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MAE of our model\n",
    "mae = np.mean(np.abs(crops[\"pred\"] - crops[\"yield\"]))\n",
    "mae # Display the MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4881fe-4afe-41df-9712-3667897ed9fa",
   "metadata": {},
   "source": [
    "This number is much easier to interpret, e.g., we can compare it to the mean of the `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae8f8a-3e93-4d2d-8435-a27e04ba1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops[\"yield\"].mean() # Display the mean of the yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87575a43-4a05-44d2-8e83-4d1b2dd227e4",
   "metadata": {},
   "source": [
    "Another useful model fit metric is the coefficient of determination, the R¬≤. The R¬≤ tells us, in percentage, how much of the variance in the dependent variable is explained by our model. Intuitively, an R¬≤ close to one (100%) fits the data well, while an R¬≤ close to zero (0%) fits the data poorly. Another good way of thinking about the R¬≤ is to know that:\n",
    "+ An R¬≤ of 100% implies that our model predicts perfectly (but not necessarily on new data!)\n",
    "+ An R¬≤ of 0% implies that our model does just as well as if we simply used the mean of the target to make our predictions.\n",
    "+ A negative R¬≤ implies that we are doing worse than if we just used the mean to make predictions.\n",
    "\n",
    "The R¬≤ is directly implemented as the `.score` method of the `LinearRegression` object in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf4fe4-ec60-4b60-b453-d76d870d5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1.score(X, y) # Display the R¬≤ of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37983354-8775-4b87-9692-e5f02b9f8f9f",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "Are you happy with the model we set up? What do you think of the results? Does it match the relationship you expected from the first task? Discuss with your classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63916835-2b87-4f45-83fa-d817c9176a5d",
   "metadata": {},
   "source": [
    "___\n",
    "## Polynomial regressions\n",
    "Perhaps you feel that the relationship between crop yield and temperature can be better represented by a curve rather than a straight line. In such a case, we can still use linear regression, despite the name indicating that it is **linear**. The trick is to introduce higher-order polynomials of the features, i.e., we can use the temperature squared.\n",
    "\n",
    "___\n",
    "#### ü§î Pause and ponder\n",
    "Why would adding a squared feature help to estimate a nonlinear relationship between temperature and crop yield? What happens if the coefficient on the *simple* feature (baseline, not squared, i.e. $x$) is positive and the coefficient on the squared feature ($x^2$) is sligthly negative? Can you try to picture it in your head? Or perhaps even sketch it on a sheet of paper?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2995cd-6203-4375-bfeb-acc5f6fcbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides a nice functionality to compute polynomial of our features\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a1018-aaa1-4c33-9bfe-20451b86f734",
   "metadata": {},
   "source": [
    "In any case, let's go ahead and extend our features with the squared temperature. Our `y` stays the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec90b5-86ac-4932-be49-b70fa6036dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 degrees of polynomials, without the constant\n",
    "poly2 = PolynomialFeatures(2, include_bias=False) \n",
    "# Define a new X with the squared feature\n",
    "X2 = poly2.fit_transform(X)\n",
    "X2[:5, :] # Check the first five rows of our new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89375-aa1e-424f-afd8-7540e17fab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new OLS object\n",
    "ols2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c814701-3cab-4bea-b4e0-8db3f121d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the estimator \n",
    "ols2.fit(X2, y)\n",
    "# Add the predictions to our `crops` dataframe\n",
    "crops[\"pred2\"] = ols2.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcaaee-92ba-4d9d-9f73-2e05c989cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our predictions\n",
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Add scatterplot\n",
    "ax.scatter(crops[\"temp\"], crops[\"yield\"], label=\"Data\")\n",
    "ax.plot(crops[\"temp\"], crops[\"pred\"], \"-o\", label=\"Prediction (1)\", color=\"orange\")\n",
    "ax.plot(crops[\"temp\"], crops[\"pred2\"], \"-o\", label=\"Prediction (2)\", color=\"green\")\n",
    "# Label axes\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Yield\")\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "# Add a legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d0fbd-3c34-4d44-b252-c8849cb6262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE of our new model\n",
    "mse2 = np.mean((crops[\"pred2\"] - crops[\"yield\"]) ** 2)\n",
    "mse2 # Display the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc2adc-1cef-44cb-8a18-596c7402170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MAE of our new model\n",
    "mae2 = np.mean(np.abs(crops[\"pred2\"] - crops[\"yield\"]))\n",
    "mae2 # Display the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7e715-dd3b-4b5d-8ee5-28abdf262b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2.score(X2, y) # Display the R¬≤ of our new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065e994-2751-49f6-bd32-0ed64b7a93ce",
   "metadata": {},
   "source": [
    "It seems hard to argue that this new model including the squared temperature does not perform better than the one with only the temperature. So, what if we add more polynomials instead of only the squared temperature? Can we do even better? Let's see...\n",
    "\n",
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 1**</font>\n",
    "\n",
    "1. Create an `X3` array with a higher-order polynomial of your liking. There is a catch, however. If you raise the values of `temp` to a high power (say 10 or 50, this will yield numbers that are higher than the numerical limits of `numpy`. So you will not get very interesting results. To prevent this, first define a function `standardize = lambda x: (x - x.mean()) / x.std()` and apply it to `X`. Then calculate the polynomial terms in analogy to the code above. Call the result `X3`. Finally, standardize all the columns of `X3` using `np.apply_along_axis(standardize,0, X3)`. Why does the standardization solve the numerical limit problem?\n",
    "2. Create an `ols3` model which uses this new (and standardized) `X3` to estimate a linear regression.\n",
    "3. Compute the MSE, MAE, R¬≤.\n",
    "4. Plot the results of your new model, compare the results to `ols1` and `ols2`.\n",
    "5. Try to find the best model. If you were tasked with predicting the effect of an increase in average temperature by, say, 1 degree celusius, which model would you choose? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7357e-8a6e-4dbc-b153-df83b8546ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548e530-bd70-44ea-833e-e6e58a60c6bb",
   "metadata": {},
   "source": [
    "___\n",
    "## Investigating model quality using a validation set\n",
    "\n",
    "We have now used different OLS models and observed how they perform differently with respect to the MSE, MAE, and R¬≤ measures. There is, however, a major caveat in the approach we have used. If you think about it, for every one of the three model specifications, we have fitted our model on the data and then used this very same data to measure how well our model performs. Why is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed43c8d-2d7e-48b6-8363-dd8458dd099f",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 2**</font>\n",
    "\n",
    "Suppose that we had built a model where we end up with predicting the exact values of the target variable, i.e. $\\hat{y}_i = y_i$ for all cases $i$. \n",
    "1. What could be problematic with such a model? **It has a MSE of zero!** So isn't it perfect?\n",
    "2. Do you think such a model with an MSE of zero exists? Can you construct one? Try for a few minutes!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bbe38-686a-4963-b097-15c5fa0ea9e9",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### Splitting the data\n",
    "We now split the data into two subsets:\n",
    "\n",
    "#### Train(ing) set  \n",
    "This is the subset of data which we use to fit our model, i.e., estimate the parameters of our models, such as the weights $\\hat{\\mathbf{w}}$. In essence, what we did above is to use the full dataset as a training set. (This is not a good practice, however!)\n",
    "\n",
    "#### Test(ing) set  \n",
    "The test set refers to the subset of data which we use to test how our model performs **out-of-sample**. Since the model was fitted on the training data, the training data results are the **in-sample** results, i.e., the results *within the trained sample*. However, from the point of view of our model, the test data is completely new, it was never seen before. So the performance on this new data indicates how well our model is able to generalize what was learned to a new dataset, i.e., we talk about **out-of-sample** performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28779e82-8d46-48b8-86d5-a9571ef4fa7b",
   "metadata": {},
   "source": [
    "So let's use some code to split our data. A good split ratio depends on how much data you have, but in general we use something like 60%-80% on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb24f1-bde1-49ae-a322-73388ae764bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn provides a nifty function to split train/test sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a8227-5657-4a96-90f6-e9dc297a58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # Display the size of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ed3a4-ed9c-40f1-b935-2a491f63aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40453455-13ee-4a13-81cb-e23af3a96feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new Xtrain and Xtest with the squared feature\n",
    "X2train = poly2.fit_transform(Xtrain)\n",
    "X2test = poly2.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf11a9-f0d7-4838-9849-2b1391c41748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new Xtrain and Xtest with the higher-order polynomial features\n",
    "# NOTE: poly3 is the polynomial from your solution to Question 1 above.\n",
    "\n",
    "Xtrain_s = standardize(Xtrain)\n",
    "X3train = poly3.fit_transform(Xtrain_s)\n",
    "X3train = np.apply_along_axis(standardize,0, X3train)\n",
    "\n",
    "Xtest_s = standardize(Xtest)\n",
    "X3test = poly3.fit_transform(Xtest_s)\n",
    "X3test = np.apply_along_axis(standardize,0, X3test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042f138-1956-4da6-82c7-e9d553dbdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape # Display the size of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71468a3-eaba-4e75-b3e7-7e42c094314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest.shape # Display the size of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fae7af-d888-4e07-9bae-0f5fb595f2b3",
   "metadata": {},
   "source": [
    "### Training the models\n",
    "Alright. We have a train set of roughly two thirds of our original dataset and a test set consisting of the rest. Let's go ahead and fit some models on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785686e3-b4ed-4a86-8124-38ba72e7d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the estimator\n",
    "ols1_t = LinearRegression() \n",
    "# Fit the estimator (‚ö†Ô∏è notice the fit on the train data only ‚ö†Ô∏è)\n",
    "ols1_t.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36feff2-075e-4f6b-9f05-a6fa71fa57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the estimator\n",
    "ols2_t = LinearRegression() \n",
    "# Fit the estimator (‚ö†Ô∏è notice the fit on the train data only ‚ö†Ô∏è)\n",
    "ols2_t.fit(X2train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13613d-ae17-492b-a15d-1c046a7efdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the estimator\n",
    "ols3_t = LinearRegression() \n",
    "# Fit the estimator (‚ö†Ô∏è notice the fit on the train data only ‚ö†Ô∏è)\n",
    "ols3_t.fit(X3train, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ec34a-0070-4550-bac8-08fcae310440",
   "metadata": {},
   "source": [
    "### Evaluating the models\n",
    "The models are set up, let us now compute the model metrics on the train and test sets in order to evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e34714-d4be-469d-8858-d05106b4eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some lists to help us compute the metrics\n",
    "model_list = [ols1_t, ols2_t, ols3_t]\n",
    "Xtrain_list = [Xtrain, X2train, X3train]\n",
    "ytrain_list = [ytrain for _ in range(3)]\n",
    "Xtest_list = [Xtest, X2test, X3test]\n",
    "ytest_list = [ytest for _ in range(3)]\n",
    "\n",
    "# # In case you are confused by the lilst comprehensions\n",
    "# print(\"ytrain:\", ytrain)\n",
    "# print(\"\\n\\n List comprehension:\\n\\n\", [ytrain for _ in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13ef8b-e4ed-409a-b525-34b127738bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to compute MSE, MAE, R2\n",
    "compute_mse = lambda m, X, y: np.mean((m.predict(X) - y) ** 2)\n",
    "compute_mae = lambda m, X, y: np.mean(np.abs(m.predict(X) - y))\n",
    "compute_r2  = lambda m, X, y: m.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef9649-75d5-41df-a125-29bebd3d7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics into lists for plotting\n",
    "# OLS with single feature\n",
    "ols1_results = {\n",
    "    \"train\": [f(ols1_t, Xtrain, ytrain) for f in [compute_mse, compute_mae, compute_r2]],\n",
    "    \"test\": [f(ols1_t, Xtest, ytest) for f in [compute_mse, compute_mae, compute_r2]]\n",
    "}\n",
    "# OLS with 2 features\n",
    "ols2_results = {\n",
    "    \"train\": [f(ols2_t, X2train, ytrain) for f in [compute_mse, compute_mae, compute_r2]],\n",
    "    \"test\": [f(ols2_t, X2test, ytest) for f in [compute_mse, compute_mae, compute_r2]]\n",
    "}\n",
    "# OLS with multiple features\n",
    "ols3_results = {\n",
    "    \"train\": [f(ols3_t, X3train, ytrain) for f in [compute_mse, compute_mae, compute_r2]],\n",
    "    \"test\": [f(ols3_t, X3test, ytest) for f in [compute_mse, compute_mae, compute_r2]]\n",
    "}\n",
    "\n",
    "# OK, this list comprehensions may feel a little dense, but you see that they are very elegant and practical. Do you understand them?\n",
    "# Here is their output:\n",
    "\n",
    "print(\"ols1: \", ols1_results)\n",
    "print(\"\\nols2: \", ols2_results)\n",
    "print(\"\\nols3: \", ols3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f48ae9-0881-4a23-b4e4-7b93e67ce4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we present these results as plots (üôÄ ü§Ø this code is quite complicated, no need to focus on it for now)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "width = .3 # Bar width\n",
    "for i in range(3):\n",
    "    labs = [f\"OLS {i}\" for i in range(1, 4)] if i == 0 else [\"\" for _ in range(1, 4)]\n",
    "    axs[i].bar(0, ols1_results[\"train\"][i], width, label=labs[0], color=\"blue\")\n",
    "    axs[i].bar(0 + width, ols2_results[\"train\"][i], width, label=labs[1], color=\"orange\")\n",
    "    axs[i].bar(0 + 2 * width, ols3_results[\"train\"][i], width, label=labs[2], color=\"green\")\n",
    "    axs[i].bar(1, ols1_results[\"test\"][i], width, color=\"blue\")\n",
    "    axs[i].bar(1 + width, ols2_results[\"test\"][i], width, color=\"orange\")\n",
    "    axs[i].bar(1 + 2 * width, ols3_results[\"test\"][i], width, color=\"green\")\n",
    "# Plot titles\n",
    "axs[0].set_title(\"Mean Squared Error\")\n",
    "axs[1].set_title(\"Mean Absolute Error\")\n",
    "axs[2].set_title(\"R¬≤\")\n",
    "# Labels and legend\n",
    "for ax in axs:\n",
    "    ax.set_xticks([width, width + 1], [\"Train\", \"Test\"])\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a6ebc-8189-44d6-8059-6446c05cd539",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 3**</font>\n",
    "\n",
    "1. What is going on in the plots? Why is OLS 3 performing this badly on the test set? Is it even possible for an R¬≤ to be negative? \n",
    "2. Is it also possible (at least theoretically) that the error on the testing set is smaller than that of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0adb34-4a79-4ef1-afae-5991ef565d5a",
   "metadata": {},
   "source": [
    "___\n",
    "## Validation and cross-validation\n",
    "It's a natural question to ask what degree of the polynomial delivers the best model in the sense of optimizing the performance\n",
    "on data that has not been used to derive the estimated (trained) weights or coefficients. This means asking the question *what is the best model out-of-sample?*.\n",
    "\n",
    "We have already talked about a **validation set** above, we are now going to explore this concept in more depth. The difference between **validation** and **testing** can be difficult to grasp at first. In fact, it is so difficult to grasp that more often than not, the terms are used interchangeably, even [Wikipedia has a section on this confusion in terminology](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets). All-in-all, understanding the difference in semantics is less important than understanding the main concept. Just be wary that when you hear about *validation* and *testing*, it might mean something different, depending on the source.\n",
    "\n",
    "In general, the process is the following:\n",
    "1. **Training**: we estimate the parameters of a model on some data.\n",
    "2. **Validation**: we select the best model (*validate*) based on how it performs on some other data that has not been used yet.\n",
    "3. **Testing**: we assess the performance of the model. At this point, the model is final, you can think of this as *observing how your chosen model might perform in the real world*.\n",
    "\n",
    "**Cross-validation** is then simply an advanced method of validation, where we split the data into training and validation sets multiple times, this generally allows us to get more robust and accurate models. Cross-validation is generally the preferred method in machine learning.\n",
    "\n",
    "`scikit-learn` provides a lot of helpful functions for machine learning, and, of course, [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) is also part of this toolkit. So let's dive into the code and see how we might use cross-validation to find the optimal number of polynomials to include in our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2933ad1-c347-4be6-b7d9-55b713a17280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score # cross-validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ba471-93e4-4362-bec8-420995173b53",
   "metadata": {},
   "source": [
    "The `cross_val_score` function is the default `scikit-learn` function for cross-validation. We will first demonstrate its use for our simple OLS model with a single variable and observe the mean squared error over the iterations of a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83dfd2-ed15-4e8a-9d28-c5735bed4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = 5, implies that we use 5 folds, the function will return \n",
    "# the negative mean absolute error for each fold, so we have to take its \n",
    "# negative value again to obtain the MAE\n",
    "mae_cv = -cross_val_score(ols1_t, Xtrain, ytrain, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "print(f\"Cross-validation mean absolute error: {np.mean(mae_cv):.2f} (¬± {np.std(mae_cv)/np.sqrt(5):.2f})\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05df0a8-1317-42ca-9139-bcfeb8df533e",
   "metadata": {},
   "source": [
    "So let's now use this to actually select the number of polynomials we want to include in our model...\n",
    "\n",
    "We will run a loop from 1 to 20 polynomials and then select the model with the lowest cross-validation MAE as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aedb99-b583-4877-a099-4ffc75df666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep track of the results\n",
    "mae_mean_list = []\n",
    "mae_se_list = []\n",
    "# Instantiate a LinearRegression model\n",
    "ols_cv = LinearRegression()\n",
    "# Loop over the polynomials\n",
    "for p in range(1, 21):\n",
    "    \n",
    "    # Compute polynomials\n",
    "    poly = PolynomialFeatures(p, include_bias=False)\n",
    "    X_cv = poly.fit_transform(Xtrain_s)\n",
    "    X_cv = np.apply_along_axis(standardize,0, X_cv)\n",
    "    \n",
    "    # Run cross-validation\n",
    "    mae_cv = -cross_val_score(ols_cv, X_cv, ytrain, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "    \n",
    "    # Store the mean and s.e. of the 5-folds\n",
    "    mae_mean_list.append(np.mean(mae_cv))\n",
    "    mae_se_list.append(np.std(mae_cv) / np.sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b7dc9-14b6-4702-811f-37d39c19825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Errorbar plot of mean MAE and standard error of the mean MAE\n",
    "ax.errorbar(range(1, 21), mae_mean_list, yerr=mae_se_list)\n",
    "# Single red dot for best result\n",
    "best = np.argmin(mae_mean_list)\n",
    "ax.scatter(best+1, mae_mean_list[best], color=\"red\", s=100, label=\"Best\")\n",
    "# Add labels, ticks, legend, grid\n",
    "ax.set_xlabel(\"Number of polynomials\")\n",
    "ax.set_ylabel(\"Mean CV MAE\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xticks(range(1, 21), range(1, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef750022-ef6c-48ed-9036-1d4a08a84f6d",
   "metadata": {},
   "source": [
    "According to our cross-validation, the best model is the one with polynomials up to the fourth order.\n",
    "\n",
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 4**</font>\n",
    "\n",
    "1. What does this actually mean, that the best model is one with a polynomial of fourth order? Is this a guarantee that the model with polynomials up to the fourth order will also perform best on a test set? \n",
    "2. What if instead of 100 data points in total we had 1'000, what about 10'000, 100'000, and 1'000'000. Would this change anything about how confident you feel about results from cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147df4ea-665b-46dc-ab90-78c4cc5f69e4",
   "metadata": {},
   "source": [
    "___\n",
    "To conclude, let's have a quick look at how this *winning* model performs on the test set, very similar to above, we'll just compare it to the *simple* OLS model and the model with polynomials of second order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a040c1-16a8-4ab1-bb8e-a7af007e6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "ols_best = LinearRegression()\n",
    "# Compute polynomials\n",
    "poly_best = PolynomialFeatures(4, include_bias=False)\n",
    "\n",
    "Xtrain_best = poly_best.fit_transform(Xtrain_s)\n",
    "Xtrain_best = np.apply_along_axis(standardize,0, Xtrain_best)\n",
    "\n",
    "Xtest_best = poly_best.fit_transform(Xtest_s)\n",
    "Xtest_best = np.apply_along_axis(standardize,0, Xtest_best)\n",
    "\n",
    "# Fit model\n",
    "ols_best.fit(Xtrain_best, ytrain)\n",
    "# OLS with 9 features\n",
    "ols_best_results = {\n",
    "    \"train\": [f(ols_best, Xtrain_best, ytrain) for f in [compute_mse, compute_mae, compute_r2]],\n",
    "    \"test\": [f(ols_best, Xtest_best, ytest) for f in [compute_mse, compute_mae, compute_r2]]\n",
    "}\n",
    "\n",
    "print(\"ols_best_results: train: \", ols_best_results[\"train\"])\n",
    "print(\"\\nols_best_results: test: \", ols_best_results[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f6536-63ae-4574-bd7b-0f14bab0e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics (üôÄ ü§Ø same code as above, no need to focus on it for now)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "width = .3 # Bar width\n",
    "for i in range(3):\n",
    "    labs = [\"OLS 1\", \"OLS 2\", \"Best CV\"] if i == 0 else [\"\" for _ in range(1, 4)]\n",
    "    axs[i].bar(0, ols1_results[\"train\"][i], width, label=labs[0], color=\"blue\")\n",
    "    axs[i].bar(0 + width, ols2_results[\"train\"][i], width, label=labs[1], color=\"orange\")\n",
    "    axs[i].bar(0 + 2 * width, ols_best_results[\"train\"][i], width, label=labs[2], color=\"green\")\n",
    "    axs[i].bar(1, ols1_results[\"test\"][i], width, color=\"blue\")\n",
    "    axs[i].bar(1 + width, ols2_results[\"test\"][i], width, color=\"orange\")\n",
    "    axs[i].bar(1 + 2 * width, ols_best_results[\"test\"][i], width, color=\"green\")\n",
    "# Plot titles\n",
    "axs[0].set_title(\"Mean Squared Error\")\n",
    "axs[1].set_title(\"Mean Absolute Error\")\n",
    "axs[2].set_title(\"R¬≤\")\n",
    "# Labels and legend\n",
    "for ax in axs:\n",
    "    ax.set_xticks([width, width + 1], [\"Train\", \"Test\"])\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cb4ee-2cc4-4e8c-a382-1b452497a2e1",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 5**</font>\n",
    "\n",
    "1. While the *best* cross-validation clearly outperforms the other two on the training set, it does not perform very well on the testing set. As a matter of fact, the model selected as optimal by the cross-validation method is really disappointing. Why could that be the case?\n",
    "2. Rerun the entire analysis with the data set `us_crops_900.csv`, instead of `us_crops.csv`. What is different? What do you conclude about cross-validation and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283903b-d616-4f49-8b9e-36ce8f55c11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
