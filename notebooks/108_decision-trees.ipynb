{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e93146-028f-40c3-8a0b-a92015dd9ce1",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/108_decision-trees.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e76e50-984c-4427-945c-8f7e04fa11ce",
   "metadata": {},
   "source": [
    "# Regression and Classification Trees\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58da3a-9a12-469a-a3d9-d95da6b1b86d",
   "metadata": {},
   "source": [
    "In this notebook, we will learn about **regression trees** and **classification trees**. More generally, such trees are also called **decision trees** as an umbrella term for both regression and classification problems since one can interpret their structure as a *decision map*.\n",
    "\n",
    "These are interesting because they represent a class of models for which we can't easily write functional form such as\n",
    "\n",
    "$$f(\\mathbf{w},\\mathbf{x}) = b + w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots$$\n",
    "\n",
    "In a sense, they are what we typically call **non-parametric** since they do not have parameters that we can adjust (in the sense of *weights*, this does not mean that there are no *hyperparameters* that can be used for model tuning!).\n",
    "\n",
    "As a standalone, regression trees are not very powerful for prediction problem. However, they have two important purposes:  \n",
    "1. They provide a nice interpretable non-parametric model to a given problem. Interpreting non-parametric models is often hard, but regression trees are an exception.\n",
    "2. They serve as building blocks to strong prediction models such as random forests and boosted trees.\n",
    "\n",
    "In this course, we focus mostly on the second of these two points. We will begin by building an understanding for regression trees in general, and, in the next notebook, we will extend this understanding to move on to random forests.\n",
    "\n",
    "For the illustration of trees, we will use a dataset of Titanic passengers and whether they survived. We will also have a look at some simulated data to study the overfitting behavior of trees.\n",
    "\n",
    "If you want to read more on tree models, consult Chapter 8 of the book [Introduction to Statistical Learning](https://www.statlearning.com/) and [this blog](http://uc-r.github.io/2018/05/09/random-forests/).\n",
    "\n",
    "\n",
    "### üßë‚Äçüíª <font color=green>**Your Task**</font>\n",
    "\n",
    "Go through the explanations and code pieces of this notebook and solve the questions outlined below. There is no need to understand things in great detail. If you just go a little bit further than scratching the surface it's perfectly fine. You should get a gut feeling about regression trees, no deep understanding needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f6ca0-f1c8-4195-afd2-9406edee0392",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing\n",
    "Just like the iris dataset, the Titanic passenger dataset is well known in the data science community. If you are not sure what a specific variable entails, you should try to figure it out by yourself using a search engine of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12c59a-40ba-4e02-a4f0-481f24b3b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Plotting\n",
    "import numpy as np # Numerical computing\n",
    "import pandas as pd # Dataframes\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d110718-be7f-4a28-b46a-f2d7f2d78e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "titanic = pd.read_csv(f\"{DATA_PATH}/data/titanic.csv\")\n",
    "titanic # Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cf6b34-fcb4-43fc-8fca-51c987e6bd03",
   "metadata": {},
   "source": [
    "As always, data cleaning comes first. We want to predict the `Survived` variable, i.e., whether our passengers have survived the accident or not. Let's look at how balanced our data is, and whether there are any missing values for the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a1761-53d5-4a6d-8a8d-ae878e844821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survivors in the dataset\n",
    "titanic[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e821c-778c-446c-b6d8-c2300b4ba80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of missing values\n",
    "titanic.isnull().sum() / titanic.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e7324-9c6f-4cb9-9e82-98a7f85c4dc3",
   "metadata": {},
   "source": [
    "We directly notice that 20% of our dataset's `Age` values are missing. Not only that but also 77% of the `Cabin` values and 0.2% of the `Embarked` values.\n",
    "\n",
    "Remember the concept of *data imputation* discussed in the last notebook? Let's go ahead and try it for the `Age` variable. We will use the mean `Age` to fill in the missing values and construct a tree using only the `Sex` and `Age` to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b51ae-aeee-41b0-bdbb-e4604be786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable, Age_Imputed and assign it with the same value as Age\n",
    "titanic[\"Age_Imputed\"] = titanic[\"Age\"]\n",
    "# Replace missing values with the mean\n",
    "titanic[\"Age_Imputed\"].fillna(titanic[\"Age\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02430d6e-a4d4-4b0e-8ed6-40fe2252fe7a",
   "metadata": {},
   "source": [
    "Because the sex is in text format, we also need to create dummies for it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a43cba-a005-4f94-9ead-594371b3fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Sex variable with its dummy\n",
    "titanic[\"Sex\"] = pd.get_dummies(titanic[\"Sex\"], drop_first=True)\n",
    "titanic # Display the data again for good measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfbd8a-6418-419e-8252-dfea24c75871",
   "metadata": {},
   "source": [
    "## A simple classification tree\n",
    "\n",
    "Let's dive right in and create our first classification tree. For this pedagogical example we will not make a train/test split yet and we will only use `Sex` and `Age_Imputed` as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebc67f-9697-4508-8aa9-71801f99be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification tree and a function to plot its contents\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d7a21-8318-4c2b-a01a-ae57816ec41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and labels\n",
    "X = titanic[[\"Sex\", \"Age_Imputed\"]]\n",
    "y = titanic[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465ff8a-93df-4e78-b2af-29c91f425818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tree model with a cost-complexity parameter for pruning of 0.01\n",
    "tree1 = DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "# Fit the model on our data\n",
    "tree1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead25c5-9955-4805-a916-c03b2ea031a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Visualize the results\n",
    "plot_tree(tree1, ax=ax, feature_names=[\"Sex\", \"Age\"], \n",
    "          class_names=[\"Dead\", \"Survived\"], label=\"root\", filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8a373-8770-4600-ad7f-75c0e022f6d9",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 1**</font>\n",
    "\n",
    "Try to read and understand the above plot.\n",
    "+ Can you figure out what each square represents and how to read the whole tree?\n",
    "+ What are the main predictors for survival according to the results of our tree?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f950c-6977-4fdb-a4c4-7a9e93f4b734",
   "metadata": {},
   "source": [
    "Let's do a second tree but with more variables. Namely, we will add the number of siblings or spouses aboard (`SibSp`), the number of parents or children aboard (`Parch`) and the passenger class (`Pclass`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe420ab-8675-409f-bd84-9c77d65924ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new X values (y stays the same)\n",
    "X2 = titanic[[\"Sex\", \"Age_Imputed\", \"SibSp\", \"Parch\", \"Pclass\"]]\n",
    "# Even though it is given as a number, Pclass is a categorical variable, thus we encode it\n",
    "X2 = pd.get_dummies(X2, columns=[\"Pclass\"])\n",
    "X2 # Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1df8d-bef5-4831-90e8-a77a88b74100",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "Have you noticed that we did not use `drop_first=True` in the `get_dummies()` function above? This is not a mistake, so why did we not do it?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076c1f3-189a-45bb-9266-18ab5d20d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tree model with a cost-complexity parameter for pruning of 0.01\n",
    "tree2 = DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "# Fit the model on our data\n",
    "tree2.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce8e82-fc23-4cf9-85c4-671e567873fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Visualize the results\n",
    "plot_tree(tree2, ax=ax, class_names=[\"Dead\", \"Survived\"], label=\"root\", filled=True,\n",
    "          feature_names=[\"Sex\", \"Age\", \"Siblings & Spouses\", \"Parents & Children\", \n",
    "                         \"1st Class\", \"2nd Class\", \"3rd Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef84e9-dd5d-474c-8141-7a54832e4129",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 2**</font>\n",
    "\n",
    "Compare this new plot with the one above. What do you notice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba5b5a-53b7-4a53-a810-84638f3db447",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 3**</font>\n",
    "\n",
    "Think about the following:\n",
    "\n",
    "1. What is the purpose of a classification (and regression) tree? What defines a *good* tree?\n",
    "2. What would be a *good* algorithm to build such a tree? Discuss the intuition behind the issues that must be taken into account when growing such a tree. Do you have an idea of such an algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cd0b8-2bf4-4045-b73a-fe31a140cadf",
   "metadata": {},
   "source": [
    "___\n",
    "## Overfitting example with regression trees\n",
    "This example is inspired from  https://rstudio-pubs-static.s3.amazonaws.com/312930_f38ab2a78ed144a9b7431f1ffcd18539.html.\n",
    "\n",
    "Have you noticed, how each time we instantiated a tree, we use `ccp_alpha = 0.01`, but what does this mean exactly? As it turns out, this is a tuning parameter which helps us avoiding that the tree overfits. In this example, we will look at what happens when this parameter is equal to zero (we don't specify it on instantiation).\n",
    "\n",
    "We begin by generating two random normal variables, $x_1$ and $x_2$. Our target outcome $y$ takes values $0$ or $1$ with the probabilities:\n",
    "\n",
    "\\begin{align}\n",
    "P(y = 1) = \\begin{cases} 0.9 \\, &\\text{ if } x_1 < 2 \\\\ 0.9 &\\text{ if } x_1 \\geq 2 \\text{ and } x_2 < -.5) \\\\ 0.1 &\\text{ otherwise.}\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058401b0-36f3-4c2e-a80d-b35c45a9cedf",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 5**</font>\n",
    "\n",
    "1. Draw the constellation for $x_1$, $x_2$ and $y$ in an $x_1$-$x_2$-space (plane).\n",
    "2. Draw a (human-made) tree for predicting $y$. Is it a complex tree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737f288-f0c8-467f-aafa-b8a00de9dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we generate data that correspond to the data-generating process outlined in this equation. No need to understand the details. Just trust that this does what it should.\n",
    "# Set random seed, number of observations\n",
    "np.random.seed(72)\n",
    "N = 1272\n",
    "# Generate variables\n",
    "x1 = np.random.randn(N)\n",
    "x2 = np.random.randn(N)\n",
    "# üôÄ ü§Ø Generate y\n",
    "y_overfit = (np.random.rand(N) < .1 + .8 * ((x1 < 2) | ((x1 >= 2) & (x2 < -.5)))).astype(int)\n",
    "# For good measure show the counts in y\n",
    "np.unique(y_overfit, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec667436-71a0-4e37-badc-16830a748214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some \"disturbances\" to confuse our regression tree, \n",
    "# note that x3 is correlated with x1, but the other variables are not\n",
    "x3 = np.random.randn(N) + x1\n",
    "x4 = np.random.randn(N) + 2\n",
    "x5 = np.random.randn(N) + .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b401435-f445-463c-9110-3b47887e9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix X \n",
    "X_overfit = np.array([x1, x2, x3, x4, x5]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17100bd0-d30e-4531-acd0-e8471ec33c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the tree WITHOUT the cost-complexity pruning parameter\n",
    "tree_overfit = DecisionTreeClassifier()\n",
    "tree_overfit.fit(X_overfit, y_overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ad4c0-91b9-411d-ab17-edbf759386f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Visualize the results\n",
    "plot_tree(tree_overfit, ax=ax, class_names=[\"0\", \"1\"], label=\"root\", filled=True,\n",
    "          feature_names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e7fef-6365-4bba-b551-f7e363a363b5",
   "metadata": {},
   "source": [
    "**That's quite the tree!** Also, this plot took quite a while to generate. Without going into too many details, this makes it clear that this cost-complexity parameter is very important for our tree to not overfit. A good idea is to proceed with cross-validation to find the optimal cost-complexity parameter!\n",
    "\n",
    "Let's go ahead and do this to see how the cross-validation error behaves depending on the cost-complexity pruning value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62906cc6-ea85-43e0-bc0b-9c42d490b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I mport cross-validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Instantiate lists to keep track of missclassification over CV rounds\n",
    "accuracy_mean = []\n",
    "accuracy_se = []\n",
    "# CV error over different values of cost-complexity\n",
    "ccp_values = [0, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, \n",
    "              0.01, 0.025, 0.05, 0.075, 0.1, .25, .5, .75, 1]\n",
    "nfolds = 10 # 10-fold CV\n",
    "for ccp in ccp_values:\n",
    "    # Create tree\n",
    "    tree_overfit = DecisionTreeClassifier(ccp_alpha=ccp)\n",
    "    acc = cross_val_score(tree_overfit, X_overfit, y_overfit, cv=nfolds, scoring=\"accuracy\")\n",
    "    accuracy_mean.append(100 * np.mean(acc))\n",
    "    accuracy_se.append(100 * np.std(acc) / np.sqrt(nfolds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540afa9-3389-41d6-b368-4e203f8c6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.errorbar(ccp_values, accuracy_mean, yerr=accuracy_se)\n",
    "# Show a red dot for the best result\n",
    "best = np.argmax(accuracy_mean)\n",
    "ax.scatter(ccp_values[best], accuracy_mean[best], color=\"red\", s=100, label=\"Best\")\n",
    "# Set the x-axis scale to be logarithmic\n",
    "ax.set_xscale(\"log\")\n",
    "# Add grid, labels, legend\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Cost-complexity pruning parameter (alpha)\")\n",
    "ax.set_ylabel(\"Mean out-of-sample accuracy\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922dc8e-ac90-4fa8-97d6-df436d1d7fac",
   "metadata": {},
   "source": [
    "Now let's reesimate and draw the tree for the optimal ccp value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803b2ab-0d37-48d6-afea-55290c8ac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_best = DecisionTreeClassifier(ccp_alpha=ccp_values[best])\n",
    "\n",
    "tree_best.fit(X_overfit, y_overfit)\n",
    "\n",
    "\n",
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Visualize the results\n",
    "plot_tree(tree_best, ax=ax, class_names=[\"0\", \"1\"], label=\"root\", filled=True,\n",
    "          feature_names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d04ef-e1a0-4085-9aed-6d7044b556bb",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 6**</font>\n",
    "\n",
    "Is this satisfactory? Did cross-validation do a good job?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1706a60-ce54-4363-873f-4989af377e65",
   "metadata": {},
   "source": [
    "___\n",
    "## Gini impurity and cross-entropy\n",
    "\n",
    "Let's create a quick and dirty plot of the shape of the **cross-entropy** function and of the **Gini impurity** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c08b47-6f90-4ea7-a292-8b0b1e6b000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Plot cross-entropy\n",
    "ax.plot(p, -p * np.log(p), label=\"Cross-entropy\")\n",
    "# Plot Gini impurity\n",
    "ax.plot(p, p * (1 - p), label=\"Gini impurity\")\n",
    "# Add grid, legend, labels, ticks\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"$p$\")\n",
    "ax.set_xticks(np.arange(11) / 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
