{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e8517d-d0a0-4cb7-8931-3641471207ea",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/207_neural-networks-in-sklearn.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2445ff8-2be7-47e3-a909-cf56debed13d",
   "metadata": {},
   "source": [
    "# Neural Networks with `scikit-learn`\n",
    "___\n",
    "\n",
    "In the previous notebook, we have seen how to write out the equations for forward passes and backpropagation to build our own neural network using only NumPy.\n",
    "\n",
    "While this is a nice didactical example, in practice, you would never code your own neural network from scratch. This would be terribly inefficient. A lot of very smart people have spent a long time figuring out how to write code to make neural network training extremely efficient, it would be a mistake to not re-use their work.\n",
    "\n",
    "In this notebook, we will very briefly show you how to use `scikit-learn` to set up a neural network for either classification or regression. **We recommend using `scikit-learn` for your project**, as this follows the same syntax of the other estimators you have seen until now. However, as your Python knowledge grows, you will realize that for neural networks, `scikit-learn` is not used. Instead, you will encounter either [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), or [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html). Those are the three dominant libraries in Python when it comes to deep learning. Unfortunately, they are a bit more complicated and require knowledge of a few somewhat advanced concepts that we have not taught you in this course. For your project, working with a known framework such as `scikit-learn` is more than enough.\n",
    "\n",
    "Lastly, note that we have not taught you `scikit-learn` because it is easy. It is, without doubt, the dominant package in Python when it comes to machine learning. The three above mentioned packages are simply particularly specialized for deep learning (i.e., neural networks with many layers), they do not offer any other type of machine learning techniques such as decision trees, random forests or clustering algorithms that `scikit-learn` implements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a2604-d86f-484d-93ce-1d7ab8c20b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the multilayer perceptron regressor and classifier from sklearn\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "# Import a helper to scale our data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a263e0-9dbe-4b0d-bd24-b0f3aada5930",
   "metadata": {},
   "source": [
    "## Classification\n",
    "___\n",
    "\n",
    "In this short example, we show how to build a neural network to classify multiple categories (virginica, setosa, and versicolors iris flowers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0a2f4-fff1-43d9-91c3-61b9093f8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the iris dataset\n",
    "df = pd.read_csv(f\"{DATA_PATH}/data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fa6c6-29c6-4427-94f5-32b911684495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a classifier which we will use to classify the species\n",
    "# Use three hidden layers, with sizes 32, 64, and 32, ReLU activation functions,\n",
    "# stochastic gradient descent optimization and a regularization paramater\n",
    "# (lambda or alpha) of 0.001, batchsizes of 32 and 1000 epochs\n",
    "nnet = MLPClassifier(hidden_layer_sizes=(32, 64, 32), activation=\"relu\", solver=\"sgd\",\n",
    "                    alpha=0.001, batch_size=32, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211a8da-3321-4c02-82a5-61ab63f0c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of our dataset\n",
    "X = df[df.columns[:-1]]\n",
    "# Output to predict\n",
    "y = df[\"species\"]\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ca682-9152-42de-bdfa-743b846d2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network\n",
    "nnet.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "ypred = nnet.predict(X)\n",
    "\n",
    "# Count the number of missclassifications\n",
    "missclassifications = ypred != y\n",
    "\n",
    "print(f\"The network missclassifies {missclassifications.sum()} flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bcc53-84a6-4890-a705-1200a46beb31",
   "metadata": {},
   "source": [
    "## Regression\n",
    "___\n",
    "\n",
    "In this short example, we show how to build a neural network to predict a continuous variable such as the crop yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f20fd5-6417-4dae-91c4-a3a70a5329f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the US crops dataset\n",
    "df = pd.read_csv(f\"{DATA_PATH}/data/us_crops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705220d3-9737-469d-8f43-374110f7ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a neural network to predict the crop yields given the temperature\n",
    "# Use three hidden layers, with sizes 32, 64, and 32, ReLU activation functions,\n",
    "# stochastic gradient descent optimization and a regularization paramater\n",
    "# (lambda or alpha) of 0.001, batchsizes of 32 and 1000 epochs\n",
    "nnet = MLPRegressor(hidden_layer_sizes=(32, 64, 32), activation=\"relu\", solver=\"sgd\",\n",
    "                    alpha=0.001, batch_size=32, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea994e-168a-4ec7-8573-e1d0f4eb4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of our dataset\n",
    "X = df[[\"temp\"]]\n",
    "# Output to predict\n",
    "y = df[\"yield\"]\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Scale the outputs (we cannot use StandardScaler() on a 1D array)\n",
    "mu, sigma = y.mean(), y.std() # We will use this to scale back to original values!\n",
    "y = (y - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e902f-0d33-4621-abfc-b768e75f06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network\n",
    "nnet.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "ypred = nnet.predict(X)\n",
    "\n",
    "# Reconstruct outputs and scale predictions\n",
    "y = y * sigma + mu\n",
    "ypred = ypred * sigma + mu\n",
    "\n",
    "# Compute the MAE\n",
    "mae = np.sum(np.abs(ypred - y))\n",
    "\n",
    "print(f\"The mean absolute error is {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ae8da-1205-4047-90ff-f6bc4833fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction on the min to the max temp (for plotting)\n",
    "xmin, xmax = X.min(), X.max()\n",
    "xs = np.linspace(X.min(), X.max(), 100)\n",
    "# Predict and scale back to original range for the plot\n",
    "ys = nnet.predict(xs.reshape(-1, 1)) * sigma + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d06b3d-6109-49f4-9b66-f3b3739107d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot the true values\n",
    "ax.scatter(df[\"temp\"], df[\"yield\"], label=\"True values\", alpha=0.8)\n",
    "# Plot the predictions\n",
    "ax.plot(\n",
    "    scaler.inverse_transform(xs.reshape(-1, 1)),\n",
    "    ys, color=\"red\", linestyle=\"dashdot\", label=\"Predictions\"\n",
    ")\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Crop yields\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a7788-5644-4969-98d9-63d619ab7d87",
   "metadata": {},
   "source": [
    "⚠️ As a last warning, neural networks are not the *End All Be All* of machine learning models. They come with many hyperparameters and choosing the right ones is a daunting task. As you might find out, a change of hyperparameters can lead to drastic changes in the model (good or bad). If your network is very unstable, a good performance might simply mean that you have a lucky seed on your validation set, be wary!\n",
    "\n",
    "In this notebook, we have chosen some hyperparameters that might be good or bad, don't just use the ones from those examples but try to find the best ones for your problem instead (*Hint*: Think about some methods we have seen to figure out which model is best!). Furthermore, scaling is incredibly important, if you try running the regression problem without scaling the output, you will see that the network is doing much worse than even the simplest estimator we covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f3efb-91f8-4444-8b5a-9a3168010414",
   "metadata": {},
   "source": [
    "#### <font style=\"color:green\">**➡️ ✏️ Question 1**</font>\n",
    "\n",
    "Using the code above as an inspiration, create either a classifier or a regressor neural network for a data set of your choice (you can pick one from the data folder). Try fitting multiple features, play around with different hyperparameters and discuss how the results compare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
