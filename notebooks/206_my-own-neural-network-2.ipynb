{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f8c000-6b86-45c5-b104-beb428e8fdc2",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/206_my-own-neural-network-2.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db70599-d8ec-41e8-bcbf-875aff11a4bd",
   "metadata": {},
   "source": [
    "# Building our own Neural Network (pt. 2)\n",
    "___\n",
    "In this notebook, we will unleash more of the unlimited power of neural networks by introducing a second output node.\n",
    "\n",
    "Unfortunately, this also comes at the cost of some extra degree of confusion, also known as matrix algebra. The main purpose is to understand what changes are necessary to make this work, compared to the previous version. \n",
    "\n",
    "Lastly, you can play around with the parameters, in particular the number of hidden nodes and the initialization range. Compare the learning progress of the network with the one used in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dddb2b-74f6-44d3-9be0-2e5521061206",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80458da3-77d9-4ce8-9727-b17e2f6267e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np # Numerical computation package\n",
    "import pandas as pd # Dataframe package\n",
    "import matplotlib.pyplot as plt # Plotting package\n",
    "import matplotlib as mpl # For colormaps\n",
    "np.random.seed(1) # Set the random seed for reproduceability\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdb4c2-a30d-458e-8d27-a5f8d84ceb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the WDBC dataset\n",
    "wdbc = pd.read_csv(f\"{DATA_PATH}/wdbc.csv\")\n",
    "# Keep only necessary columns: the diagnosis, the perimeter, and the severity of concave portions\n",
    "# of the cell nucleus\n",
    "wdbc = wdbc[[\"perimeterM\", \"concaveM\", \"diagnosis\"]]\n",
    "wdbc # Display the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbacc68-c43e-4289-90a9-3269d343bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the necessity to standardize our inputs!\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cfea5-572e-4afc-af1e-cd15fe9b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features\n",
    "X = np.array(wdbc[[\"perimeterM\", \"concaveM\"]])\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65c4a7-5e28-4ba0-ad76-81780a55b610",
   "metadata": {},
   "source": [
    "Notice how we didn't create the labels above? This is because this time, our labels are going to be *one-hot-encoded*, i.e., each individual target $y^{(i)}$ is a vector $[0\\, 1]$ or $[1\\, 0]$, such that\n",
    "\n",
    "$$\\mathbf{Y} = \\begin{bmatrix}0 \\, 1 \\\\ 0 \\,1 \\\\ 1\\,0 \\\\ \\dots\\\\ 1\\,0 \\end{bmatrix}$$\n",
    "\n",
    "is now a matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703ed65-abbe-40bc-9e2a-40d79a7e122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one-hot-encoded target\n",
    "Y = np.array(pd.get_dummies(wdbc[\"diagnosis\"])).astype(float)\n",
    "Y # Display the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80aca4a-2309-409a-a1da-b933a4bcc8fc",
   "metadata": {},
   "source": [
    "___\n",
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f67a7-9506-464e-9255-f61d1ae3de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some parameters\n",
    "N, K = X.shape # Number of observations and features\n",
    "L = Y.shape[1] # Number of outputs\n",
    "NZ = 3 # Number of hidden nodes\n",
    "epochs = 100 # Number of training epochs\n",
    "eta = 0.01 # Learning rate\n",
    "init_range = [-.5, .5] # The range of our uniform distribution for weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def97eb4-abc5-4523-85d3-22f431a66195",
   "metadata": {},
   "source": [
    "Notice how now we keep track of the number of outputs. Now $\\mathbf{W}_2$ will not be $N_Z \\times 1$ anymore. Instead, it will be $N_Z \\times L$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ae716-5804-4f1e-8a89-5067b249d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weight matrices\n",
    "np.random.seed(72) # Set seed\n",
    "W1 = np.random.rand(K, NZ) # Randomly initialize W1\n",
    "W1 = W1 * (init_range[1] - init_range[0]) - init_range[0] # Constrain to range\n",
    "W2 = np.random.rand(NZ, L) # Randomly initialize W2\n",
    "W2 = W2 * (init_range[1] - init_range[0]) - init_range[0] # Constrain to range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e8434-01cd-4950-a3a6-357df08702ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the sigmoid function as the activation function\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f7a42-7225-4934-9fdf-9eebe4c65a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass helper\n",
    "def forward_pass(X, W1, W2):\n",
    "    # Compute the matrix multiplication of the input layer\n",
    "    S = X @ W1\n",
    "    \n",
    "    # Pass through the non-linear activation function\n",
    "    Z = sigmoid(S)\n",
    "    \n",
    "    # Compute the matrix multiplication of the hidden layer\n",
    "    T = Z @ W2\n",
    "    \n",
    "    # Pass through the non-linear activation function \n",
    "    # (This should always be sigmoid for the output to be (0, 1))\n",
    "    # ‚ö†Ô∏è Notice how we don't flatten the output anymore, this time we want to keep it a matrix!\n",
    "    return sigmoid(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a04f42-570e-41cf-a80b-5290bba3d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the output of a forward pass is indeed an NxL matrix\n",
    "pd.DataFrame(forward_pass(X, W1, W2), columns=[f\"Y{i}\" for i in range(L)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb68034-d0a6-46e7-8042-b19059e8502a",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "While the code for the loss is the exact same, the calculation of the loss is now somewhat more tricky. Recall from the math derivations that the loss function has two terms, one for $L=0$, and one for $L=1$. We must first calculate the two loss terms separately and add them together.\n",
    "\n",
    "Fortunately, using `numpy`, we don't need to think about all of this... it just works. But be careful, it's a good idea to take time and see what each part of the loss looks like since we are now dealing with a matrix instead of a vector of targets.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2eace5-ee99-4a0f-b833-1ab48ed65af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, we will use to evaluate our predictions\n",
    "def eval_predictions(X, Y, W1, W2):\n",
    "    # Use the forward_pass function defined above to compute our estimated probability\n",
    "    prob = forward_pass(X, W1, W2)\n",
    "    \n",
    "    # To avoid log(0), clip the probability to not be exactly zero or one\n",
    "    prob = np.clip(prob, 1e-8, 1-1e-8)\n",
    "    # Calculate the loss function (negative log-likelihood in this case)\n",
    "    loss = - np.sum(Y * np.log(prob) + (1 - Y) * np.log(1 - prob))\n",
    "\n",
    "    # For the actual prediction, we select the outcome with the highest probability. \n",
    "    # Unlike in the previous case, the two probabilities need not add up to 1!\n",
    "    pred = prob.argmax(1) # Take the argmax along the second axis\n",
    "    \n",
    "    # Compute number of misclassification\n",
    "    misclassifications = np.mean(pred != Y.argmax(1))\n",
    "    \n",
    "    # Output results as a dictionary\n",
    "    return {\n",
    "        \"loss\": loss, \n",
    "        \"misclassifications\": misclassifications, \n",
    "        \"prob\": prob, \n",
    "        \"pred\": pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b499c96-51c3-4375-a8c0-1999031f0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializitation of lists for bookkeeping\n",
    "loss_list = []\n",
    "misclassification_list = []\n",
    "\n",
    "# Create an array of indices (which we will shuffle later on) through which we \n",
    "# will iterate. This represent the index of the observations\n",
    "indices = np.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41be46-c0a9-47c9-ae67-647b8ffca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this, we use it to print nicely without creating too many lines\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e3190-9d8d-43c5-891f-9eb94b337d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(72) # Reset random seed for reproduceability\n",
    "# Compute the loss and misclassifications BEFORE training\n",
    "res = eval_predictions(X, Y, W1, W2)\n",
    "\n",
    "# Append to our result lists\n",
    "loss_list.append(res[\"loss\"])\n",
    "misclassification_list.append(res[\"misclassifications\"])\n",
    "\n",
    "# Run the full training loop (iterate over the number of training epochs)\n",
    "for epoch in range(epochs):\n",
    "    # Reshuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Iterate through each single data point\n",
    "    for i in indices:\n",
    "        # Note that we use [i:i+1, :] instead of [i, :] to keep it as a 1x2 matrix\n",
    "        Xi = X[i:i+1, :] # Extract features for ith observation,\n",
    "        Yi = Y[i:i+1, :] # Extract label for ith observation\n",
    "        \n",
    "        # ----- Forward pass -----\n",
    "        # Computes the predictions using Xi, W1, W2. Here we avoid using the\n",
    "        # forward_pass function because we need the hidden nodes values Zi\n",
    "        # for backpropagation. So, instead, we repeat the code (ugh...)\n",
    "        \n",
    "        # Pass to the hidden nodes (pre-activation)\n",
    "        Si = Xi @ W1\n",
    "        # Compute activation function\n",
    "        Zi = sigmoid(Si)\n",
    "        \n",
    "        # Pass to the output nodes\n",
    "        Ti = Zi @ W2\n",
    "        # Compute sigmoid probability transformation\n",
    "        prob_i = sigmoid(Ti)\n",
    "        # ‚ö†Ô∏è Notice that we now have two probabilities!!\n",
    "        \n",
    "        \n",
    "        # ----- Backward pass -----\n",
    "        # ‚ö†Ô∏è Since we have two output nodes, and two probabilities for prediction\n",
    "        # we will also have two errors!!\n",
    "        error_i = Yi - prob_i\n",
    "        \n",
    "        # Compute the gradient w.r.t. W2. ‚ö†Ô∏è W2 is a vector! See math derivations\n",
    "        grad2_i = -Zi.T @ error_i # ‚ö†Ô∏è error_i is now 1 x L, and Zi is NZ x 1\n",
    "        # Compute the gradient w.r.t. W1. ‚ö†Ô∏è W1 is matrix! Making things even worse\n",
    "        grad1_i = -Xi.T @ (error_i @ (W2.T * Zi * (1 - Zi))) # ‚ö†Ô∏è error_i is now 1 x L\n",
    "        \n",
    "        # Updating: Move 'eta' units in the direction of the negative gradient\n",
    "        W1 -= eta * grad1_i\n",
    "        W2 -= eta * grad2_i\n",
    "    \n",
    "    # Evaluate the learning process and store the results into our lists\n",
    "    res = eval_predictions(X, Y, W1, W2)\n",
    "    loss_list.append(res[\"loss\"])\n",
    "    misclassification_list.append(res[\"misclassifications\"])    \n",
    "        \n",
    "    # Print the current status (üôÄ ü§Ø ignore this part!)\n",
    "    bar = \"\".join([\"#\" if epoch >= t * (epochs // 50) else \" \" for t in range(50)])\n",
    "    stdout.write(f\"\\rEpoch: {epoch+1:>{int(np.floor(np.log10(epochs))+1)}}/{epochs} [{bar}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209d4a7-132e-4667-bdaa-2f12badf6448",
   "metadata": {},
   "source": [
    "___\n",
    "## Evaluating the results\n",
    "That's it, we can go ahead and look at the results of our own custom neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1b132-6a1a-48e9-a98f-674b01a3f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# Plot the loss over the epochs (epoch 0 is before training!)\n",
    "axs[0].plot(range(len(loss_list)), loss_list)\n",
    "# Plot the misclassification rate over the epochs\n",
    "axs[1].plot(range(len(misclassification_list)), misclassification_list)\n",
    "# Add title, grid, axis labels\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Epoch number\")\n",
    "axs[0].set_ylabel(\"Loss (negative log-likelihood)\")\n",
    "axs[0].set_title(\"Evolution of loss function over training epochs\")\n",
    "axs[1].set_ylabel(\"Misclassification rate\")\n",
    "axs[1].set_title(\"Evolution of missclassification rate over training epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c3c2a-b0d7-4316-838c-d592f0f7e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the weight matrix from the input layer to the hidden layer\n",
    "pd.DataFrame(W1, columns=[f\"Z{i}\" for i in range(NZ)], index=[\"X1\", \"X2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24957d-2d4d-4c93-97f5-55c355a971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the weight matrix  from the hidden layer to the output layer\n",
    "pd.DataFrame(W2, columns=[\"Y0\", \"Y1\"], index=[f\"Z{i}\" for i in range(NZ)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad9c48-5f7d-4c2f-9f34-49234d260ae0",
   "metadata": {},
   "source": [
    "___\n",
    "## Visualizing the decision regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0085f8-ae63-42f5-b632-a46f00b0f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define granularity and limits of grids\n",
    "npoints = 200\n",
    "x1_lims = (X[:, 0].min() - .1, X[:, 0].max() + .1)\n",
    "x2_lims = (X[:, 1].min() - .1, X[:, 1].max() + .1)\n",
    "\n",
    "# Create the x1 and x2 arrays\n",
    "x1 = np.linspace(*x1_lims, num=npoints)\n",
    "x2 = np.linspace(*x2_lims, num=npoints)\n",
    "preds = np.empty((npoints, npoints))\n",
    "\n",
    "# Compute the predictions of our network for every single combination\n",
    "for i in range(x2.shape[0]):\n",
    "    for j in range(x1.shape[0]):\n",
    "        # Create the 1x2 input\n",
    "        xi = np.array([[x1[j], x2[i]]])\n",
    "        # Compute the prediction of the network \n",
    "        # (üôÄ ü§Ø use a trick to keep the uncertainty regions...)\n",
    "        probs = forward_pass(xi, W1, W2).flatten()\n",
    "        argmax = probs.argmax()\n",
    "        preds[i, j] = (1 - argmax) * (1 - probs[0]) + argmax * probs[1]\n",
    "\n",
    "# Small trick to scale back values to their value before standardization\n",
    "Xt = scaler.inverse_transform(np.array([x1, x2]).T)\n",
    "x1, x2 = Xt[:, 0], Xt[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a716363-0732-404b-a0da-d8280f1e47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Add decision regions\n",
    "ax.contourf(x1, x2, preds, cmap=mpl.colormaps[\"coolwarm\"], alpha=0.8)\n",
    "\n",
    "# Add true data points\n",
    "for diag in [\"B\", \"M\"]:\n",
    "    subset = wdbc.loc[wdbc[\"diagnosis\"] == diag]\n",
    "    ax.scatter(subset[\"perimeterM\"], subset[\"concaveM\"], label=diag, alpha=0.9)\n",
    "\n",
    "# Add grid, labels, etc.\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Perimeter\")\n",
    "ax.set_ylabel(\"Concavity\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c2b06-b850-4252-b0f2-7dc27442fec8",
   "metadata": {},
   "source": [
    "___\n",
    "### Exercises\n",
    "\n",
    "#### <font style=\"color: green\">**‚û°Ô∏è ‚úèÔ∏è Question 1**</font>\n",
    "Try playing around with the parameters. Change the hidden nodes, learning rate, number of epochs.\n",
    "\n",
    "How do the results change? Do they get better or worse? How do the decision region change?\n",
    "\n",
    "*Hint*: If you encounter an error, try changing some parameters back. A too high learning rate can cause your weights to explode and `numpy` won't be able to handle such large numbers (in particular when trying to exponentiate them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdb29e-c0ae-44e5-9e82-d053de89c767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
