{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8775dfa1-c93e-42c6-9a5c-6dc4067497c5",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/007_joins-and-merges.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3f4b8-625c-40f6-b8c2-4d719fde99bb",
   "metadata": {},
   "source": [
    "# Joining and Merging Datasets\n",
    "___\n",
    "In this notebook, we will cover how to join and merge datasets together. While you will not use *joining* and *merging* a lot in the notebooks we view in class, because the data we provide you with is relatively clean, it is likely to be amongst the tools you use the most *in the wild*, i.e., when doing your own research or working as a data scientist somewhere. ⚠️ **You will definitely use it a lot in your projects!** ⚠️\n",
    "\n",
    "Ofentimes, data stems from multiple sources and is scattered among multiple individual files. You need to be able to combine those files and sources into one big dataset which you can use for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc2092-7371-4766-956d-41eeffd2cbca",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing\n",
    "Let's first load and inspect two of our SNB datasets, the exchange rates and trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee5f5c-3717-442c-bf26-e79e0972d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae97898-c526-464a-94a3-72132459f1d1",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 1\n",
    "\n",
    "Read in the two following datasets:\n",
    "+ The exchange rate information from **data/snb-xrates-wide.csv** into a dataframe called `snb_xr`\n",
    "+ The trades information from **data/snb-trades-wide.csv** into a dataframe called `snb_t`\n",
    "\n",
    "Inspect the datasets using the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77608221-2cbb-4c08-9cf2-ee18edb8e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3b60c-fc21-46af-b001-7e8a885fc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect exchange rates\n",
    "snb_xr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2a31c-c335-4b9e-af01-5bb39298f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect trades\n",
    "snb_t.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8aafc-edc1-4d8a-a095-6fa75cf69264",
   "metadata": {},
   "source": [
    "___\n",
    "## Merging data\n",
    "As can be taken from the official documentation, [joining and merging is a complex topic](https://pandas.pydata.org/docs/user_guide/merging.html), depending on what you are aiming to do, you need to use different specifications.\n",
    "\n",
    "In particular, we see that the `snb_t` dataframe has dates going back to 1997, while the `snb_xr` only has dates going back to 2001. If we try to combine these two dataframes, we must make a judgment call on how to handle the years which are covered by one dataset and not the other, i.e., what do we do with our data between 1997 and 2001?\n",
    "\n",
    "Furthermore, we need a column that is present in both dataframes, this is called the **key**. In our case, it's an easy choice, it's clear that the key is the date, i.e., when joining both dataframes, we will end up with a dataframe that still has only one date, but now has all the columns of both dataframes. It's also easy because our key has the same name in both dataframes, but if you look at the documentation, you will see that we can also deal with keys that have different names (as long as the contents are the same!)\n",
    "\n",
    "The way `pandas` provide merging/joining is through the function\n",
    "\n",
    "```python\n",
    "pd.merge(left_df, right_df, how=\"inner\")\n",
    "```\n",
    "\n",
    "Basically, this function takes two dataframes, a *left* one, and a *right* one (this is standard nomenclature and comes from database joins, e.g., using SQL). The `how` keyword then governs *how* the data is merge. Here is a short overview of the four most important merges you will see:\n",
    "\n",
    "|`how`|Effect|\n",
    "|--:|:--|\n",
    "|`left`|The key in `left_df` is the one that governs the merge. At the end, your merged dataframe will have **exactly** the same keys as the `left_df`, if `right_df` has some key values which are not present in `left_df`, those rows will be dropped.|\n",
    "|`right`|The exact same thing as above, but with the roles of `left_df` and `right_df` inversed.|\n",
    "|`inner`|The keys in the final dataframe are the keys which are present **in both dataframes**, i.e., we drop every observation that has only a key in either `left_df` or `right_df` but not in both!|\n",
    "|`outer`|This time we keep all the keys, as long as they appear somewhere, we will keep them in our final dataframe.|\n",
    "\n",
    "This might all sound a bit abstract, so let's start with a very simple toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2f9d8-5e4e-458f-ab4b-e7f150513cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a first dataframe\n",
    "students_gpa = pd.read_csv(f\"{DATA_PATH}/students_gpa.csv\")\n",
    "students_gpa # Show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447bf29-6a93-41d2-b9ff-bc95a2d623b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a second dataframe\n",
    "students_track = pd.read_csv(f\"{DATA_PATH}/students_track.csv\")\n",
    "students_track # Show the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173b5e7-73dc-40a2-88aa-1f5880cbceb1",
   "metadata": {},
   "source": [
    "So we have 4 students for which we have the GPA, and 5 for which we have the track, but we only have an overlap of 3 students amongst both dataframes. This is something very typical of what you will encounter in your work and research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11555a47-a26d-477d-8979-4bdb1fd57e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join with GPA as left dataframe\n",
    "pd.merge(students_gpa, students_track, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac2888-6f63-4f43-b96b-164aeb1184ff",
   "metadata": {},
   "source": [
    "See what happens? Our dataframe has the exact same **key** (students) as the left dataframe because we used a **left** join. For all students for which we don't have an observation in the `students_track` dataframe, the value is simply a `NaN`, i.e., *not a number*, an indication of a missing value!\n",
    "\n",
    "Note that we didn't specify the key, but because the dataframes had a column with the same name (`student`), these was used as a key! We could have specified the key also, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d691293-8ffa-4d3b-9b56-a8052e7da6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join with student as right dataframe\n",
    "pd.merge(students_gpa, students_track, how=\"right\", on=\"student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc9bc7-91b9-43a4-b128-891109434086",
   "metadata": {},
   "source": [
    "This is similar but now we have all `track` values filled, because we used the `students_track` dataframe as the *base* for our merge (the *right* dataframe in a *right* merge!)\n",
    "\n",
    "Note that, as in life, *left* and *right* are relative concepts. We can achieve the exact same result as above using a left join with the left dataframe being `students_track`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa0a29-c2ee-49e0-afcf-4d805c676d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact same result but with a left join, this time changing the left and right dataframe!\n",
    "pd.merge(students_track, students_gpa, how=\"left\", on=\"student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f807c52-0743-4711-8204-87279101ce97",
   "metadata": {},
   "source": [
    "Okay, all that is left is to have a look at **inner** and **outer** join. Do you already understand what they are supposed to do? Notice that the left and right dataframe does not matter in the context of inner and outer joins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c4a1b-183c-4bd0-bba7-96ed23d17834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First an inner join\n",
    "pd.merge(students_gpa, students_track, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ddc35-867f-4ee9-9d54-aad8d36e38e6",
   "metadata": {},
   "source": [
    "An *inner* join only keeps observations that are present in **both** dataframes! There are only two of them. The nice thing is that our final data is clean, i.e., we have no `NaN`, because we discarded all those observations that would have resulted in missing information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211b509-bc01-4903-a9e7-e2f4dd6b4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And lastly, an outer join\n",
    "pd.merge(students_gpa, students_track, how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbee77-b889-42e4-9e6a-156b9f743198",
   "metadata": {},
   "source": [
    "And now, using an *outer* join, we keep **everything**, it doesn't matter if they are only present in one dataframe! We end up with a dataframe with many `NaN` but it also has a lot of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a55b17-402e-43f8-8b97-4fbf4f75c650",
   "metadata": {},
   "source": [
    "___\n",
    "#### 🤔 Pause and ponder\n",
    "Is there a join which is better than the others? Can you think of a scenario where you prefer a left/right join? What about an inner join or an outer join?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed967212-f9b2-4417-83cc-95b8531eabab",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "Alright, enough reading. Let's get our hands dirty and play around with joins involving real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba9803-d5c3-4860-a383-08672ecc9a5b",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 2\n",
    "\n",
    "For each SNB dataset read in, create a `_small` version:  \n",
    "+ `snb_xr_small` should contain only three columns: `Date`, `EUR1`, and `USD1`\n",
    "+ `snb_t_small` should contain only three columns: `Date`, `A_GT_WMF`, and `E_GT_WMF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280d2a0-72e8-4764-b108-ac1bc2868c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a28b1-0f1a-4833-838d-5b5f5aefeaf0",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 3\n",
    "\n",
    "Perform a **left** join, where `snb_xr_small` is the left dataframe, and `snb_t_small` is the right dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76d1c2-bddb-4625-be50-91432baf5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb09ac-1c4e-444f-aea0-7d0e1e945507",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 4\n",
    "\n",
    "Perform a **right** join, where `snb_xr_small` is the left dataframe, and `snb_t_small` is the right dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dcf94-993d-4333-8981-2dba05555569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98500a4f-b7ab-40f8-9488-c4b6f27e6441",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ Task 5\n",
    "\n",
    "Compare the results of task 3 and task 4. What can you say about the difference? In particular look at the size of your dataframes and the values in the first and last rows!\n",
    "\n",
    "Perform an **inner** join. Can you figure out what the size of the resulting dataframe will be before even running the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d1cbb-bb1b-401e-856d-289883404679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
