{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d2f4a4-e277-47db-bffd-8c9a5515fa7c",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/204-clustering.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812dfee3-532e-4935-8fd3-8a0f61d77151",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac9c59-67f7-483a-b216-60a17f7396fd",
   "metadata": {},
   "source": [
    "In this notebook, we will go over the concept of *clustering* in machine learning. In general, clustering belongs to a category of learning which we call **unsupervised learning**. Until now, the learning algorithms we have covered consisted in training a model using *labeled data*, i.e., we had a target label (e.g., the value of a house, the species of an iris, etc.) which we were trying to predict. This is what we call **supervised learning**. Indeed, you can think of our error function as a *supervisor* telling our model how well it is performing.\n",
    "\n",
    "In unsupervised learning, our data is not labeled, so what is the goal? **Unsupervised learning** aims to learn about **relationships within the data**. As we will also see, unsupervised learning can be a useful step when we are doing supervised learning. In fact, you have already learned about an unsupervised learning algorithm: **principal component analysis**. Think about it, PCA knew nothing about the labels in our data, yet it was able to create variables that explain the variance in the data particularly well.\n",
    "\n",
    "This might still sound a bit abstract for now, but there are a lot of use cases for unsupervised learning. For instance, if we want to reduce the dimensionality of our data to improve our training time in a supervised learning framework; or maybe we are making a recommendation system for an online shop and we want to find similar buying behaviors amongst customers; or perhaps we are trying to classify topics of news articles such that *similar* articles are grouped together. \n",
    "\n",
    "This notebook will focus on **clustering**, i.e., grouping *similar* points of data into clusters. In general, we differentiate between **hard clustering**, where we assign points to a specific cluster, and **soft clustering** where we assign a *probability* of a point being in each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206882f1-7333-4fa0-9498-c6e0cc28d39a",
   "metadata": {},
   "source": [
    "___\n",
    "## Centroid-based Clustering\n",
    "\n",
    "**Centroid-based clustering** is one of the typical methods to build clusters. The general idea behind it is to associate each cluster with a **centre** and the simplest algorithm is known as **K-means clustering**. With K-means clustering, we choose a number of cluster centres $k$ and we assign the data points to the cluster with the *closest* center*.\n",
    "\n",
    "### 🙀 🤯 Mathematical Formulation\n",
    "\n",
    "Let $\\mathbf{x}_1, \\dots, \\mathbf{x}_n$ be our data points. We want to pick $k$ cluster centres $\\mathbf{c}_1, \\dots, \\mathbf{c}_k$ and assign each $\\mathbf{x}_i$ to a cluster $\\mathbf{z}_i \\in \\{1, \\dots, k\\}$ in order to minimize\n",
    "\n",
    "$$f(\\mathbf{c}_{1:k}, \\mathbf{z}_{1:n}) = \\sum_{i=1}^n d(\\mathbf{x}_i, \\mathbf{c}_{z_i}),$$\n",
    "\n",
    "where $d(\\cdot, \\cdot)$ is a distance function.\n",
    "\n",
    "Unfortunately, this type of minimization is an NP-hard problem in general (NP stands for *non-deterministic polynomial time* and NP-hard means that the problem is at least as hard as the hardest problems in non-deterministic polynomial time... in short: **this is a extremely difficult problem**).\n",
    "\n",
    "K-means uses **coordinate descent** to find a **local minimum**, as you already know, finding local minima is generally much easier than finding global ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45aa057-f10d-451c-b867-b2094caf3ab9",
   "metadata": {},
   "source": [
    "Let us start with an example of K-means on some data we already know fairly well, the iris flowers data set.\n",
    "\n",
    "Assume the following situation: you know that there are three different type of flowers in your data, but given any data point, you don't know what kind of species it belongs to. You can think of this as having the iris data set but **without** the `species` column. For plotting purposes, we will keep it in, but you can consider it to unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf035d-945b-4c10-92d4-ed08ba153d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans # K-means algorithm from scikit-learn\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdc3ac-dd53-42ac-bdbf-070fe3be1631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris data set\n",
    "iris = pd.read_csv(f\"{DATA_PATH}/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947eefe-fb5b-4d9f-86c4-39b5b17e8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.scatter(iris[\"sepal length (cm)\"], iris[\"sepal width (cm)\"], alpha=0.6)\n",
    "    \n",
    "ax.set_xlabel(\"Sepal length (cm)\")\n",
    "ax.set_ylabel(\"Sepal width (cm)\")\n",
    "ax.grid(linestyle=\"dashdot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed93688-3e21-4a19-a744-951c72ddb1f8",
   "metadata": {},
   "source": [
    "Look at the above picture, illustrating the single flowers based on their sepal length and sepal width. If this was our only information, it would be fairly difficult to classify the flowers into three clusters. We might be able to separate one cluster at the top left, but the two other clusters are harder to define. Luckily for us, K-means can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d9d6b-f497-4e56-bf3a-e878c234fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "X = np.asarray(iris[[\"sepal length (cm)\", \"sepal width (cm)\"]])\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fc9e7-acc8-45ea-b7ea-cfcb40696e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this custom function to plot your kmeans classification.\n",
    "# No need to focus on the code, the important part is the resulting plot!\n",
    "def plot_kmeans(kmeans, X, h=0.01):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # Plot the decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Obtain labels for each point in mesh\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.imshow(\n",
    "        Z, \n",
    "        interpolation=\"nearest\", \n",
    "        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "        cmap=mpl.colormaps[\"viridis\"],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        alpha=0.5\n",
    "    )\n",
    "    # Plot the scatter\n",
    "    ax.scatter(X[:, 0], X[:, 1], color=\"black\", edgecolor=\"white\")\n",
    "    # Plot the centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], marker=\"X\", s=180, linewidth=1, color=\"black\", edgecolor=\"white\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5c909-ffac-481d-adba-6d9d9935a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56249adb-8c6d-459e-87be-096076d3df01",
   "metadata": {},
   "source": [
    "The above plot shows how K-means clusters our data. Of course, K-means is somewhat random (i.e., it depends on where we set the cluster centroids to begin with), so using another `random_state` might yield different results.\n",
    "\n",
    "K-means is an iterative procedure, meaning it rearranges the centroids iteration by iteration, the following GIF illustrates this process:\n",
    "\n",
    "![](https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data/images/kmeans.gif)\n",
    "\n",
    "We see that, in this case, the clusters do not change much past the first ten iterations. Furthermore, we see that **without any knowledge of the species**, and based uniquely on the sepal length and sepal width, K-means creates cluster that contain mostly observations of the same species. On the one hand, this is quite impressive, on the other hand, it's a clear indication that amongst a particular iris species, there are similarities in the sepal widths and sepal lengths!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fde54-d9f3-4002-9b99-65740fa8eafe",
   "metadata": {},
   "source": [
    "___\n",
    "## Soft Clustering\n",
    "\n",
    "\n",
    "K-means is (typically) fast, but, in some situations, we might prefer probabilistic statements about clustering. Consider the results of the clustering for the iris data set, how confident do you feel about a point near the cluster's boundary belonging to a specific cluster vs a point far away?\n",
    "\n",
    "Soft clustering, i.e., quantifying the probability that a data point belongs to a specific cluster, helps pin down this uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d9b29-c676-4292-a2f3-34cdd03edc20",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️ <font style=\"color: green\">**Question 1**</font>\n",
    "\n",
    "Repeat the same analysis for the crabs dataset. Plot your results using the function provided above. What are your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08872937-a37e-4dfb-9600-8bfc3eed08f9",
   "metadata": {},
   "source": [
    "___\n",
    "### Mixture models\n",
    "\n",
    "Mixture models are a popular way to cluster data in a probabilistic way. Such a model takes the form\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\sum_{k=1}^K w_k p_k(\\mathbf{x}).$$\n",
    "\n",
    "The main idea behind this model is that the data $\\mathbf{x}$ is generated by\n",
    "+ selecting a mixture component $k$ with probability $w_k$,\n",
    "+ generating $\\mathbf{x}$ from the distribution associated with $p_k$.\n",
    "\n",
    "Our goal is to learn $w_1, \\dots w_K$ and the parameters associated with each component. Notice, that we assume that there are $K$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c3e31-b06b-45c2-a00e-3ea16bdbad2d",
   "metadata": {},
   "source": [
    "#### 🙀 🤯 Example: Gaussian mixture model\n",
    "\n",
    "Let $K=3$ and $\\mathbf{x} \\in \\mathbb{R}^2$, with $p_k(\\mathbf{x})$ a multivariate Gaussian with mean $\\mathbf{\\mu}_k$ and covariance $\\mathbf{\\Sigma}_k$.\n",
    "\n",
    "We have \n",
    "\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K w_k \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k\\right)$$\n",
    "\n",
    "Say that we have $n=1\\,000$ data points generated from the mixture model:\n",
    "\n",
    "![ ](https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data/images/gaussianmixture1.png)\n",
    "\n",
    "can we learn $\\mathbf{w}_{1:K}$, $\\mathbf{\\mu}_{1:K}$, and $\\mathbf{\\Sigma}_{1:K}$?\n",
    "\n",
    "\n",
    "But what if instead, our data looks like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data/images/gaussianmixture2.png)\n",
    "\n",
    "can we learn $\\mathbf{w}_{1:K}$, $\\mathbf{\\mu}_{1:K}$, and $\\mathbf{\\Sigma}_{1:K}$ now?\n",
    "\n",
    "Let's find out using scikit-learn's `GaussianMixture` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29014ac1-94a1-456d-9db3-ea2352cb5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0375ac-faeb-4e05-90d7-d43ad8c9aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data points from the first plot\n",
    "df = pd.read_csv(f\"{DATA_PATH}/gaussian_mixture1.csv\")\n",
    "df # Display the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4da4d-0650-41e4-ab11-3e53cac7bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Gaussian mixture representation with 3 components\n",
    "gaussian_mixture = GaussianMixture(n_components=3, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b310065-9dac-4253-a498-2648aef6af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the data points\n",
    "gaussian_mixture.fit(df[[\"X\", \"Y\"]])\n",
    "# Predict the cluster using our Gaussian mixture\n",
    "df[\"Cluster\"] = gaussian_mixture.predict(df[[\"X\", \"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26bc1c-deec-456c-9db2-71e279d5c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Iterate over the predicted clusters\n",
    "for cluster in df[\"Cluster\"].unique():\n",
    "    # Subset the dataframe\n",
    "    dft = df[df[\"Cluster\"] == cluster]\n",
    "    \n",
    "    # Make a scatterplot\n",
    "    ax.scatter(dft[\"X\"], dft[\"Y\"], label=cluster, alpha=0.8)\n",
    "    \n",
    "# Prettify the plot\n",
    "ax.legend(title=\"Predicted cluster\")\n",
    "ax.grid(linestyle=\"dashdot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25089e1c-e0d5-4d3c-aff9-4a693bdf4f61",
   "metadata": {},
   "source": [
    "Compare this to the clusters of the picture above, they are just the same! Be careful, however, the colors do not need to match, the important part is that the **clusters** match, i.e., the singular points are correctly identified as belonging to the same cluster.\n",
    "\n",
    "That's pretty neat, but does it also hold for the second image with more complicated data structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476fe29-54df-4980-97c5-35327f1423c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data points from the second plot\n",
    "df = pd.read_csv(f\"{DATA_PATH}/gaussian_mixture2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184efdbd-6e29-4047-8565-dfa9847cc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the data points\n",
    "gaussian_mixture.fit(df[[\"X\", \"Y\"]])\n",
    "# Predict the cluster using our Gaussian mixture\n",
    "df[\"Cluster\"] = gaussian_mixture.predict(df[[\"X\", \"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca715929-ca13-476d-b256-c6272b275db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Iterate over the predicted clusters\n",
    "for cluster in df[\"Cluster\"].unique():\n",
    "    # Subset the dataframe\n",
    "    dft = df[df[\"Cluster\"] == cluster]\n",
    "    \n",
    "    # Make a scatterplot\n",
    "    ax.scatter(dft[\"X\"], dft[\"Y\"], label=cluster, alpha=0.8)\n",
    "    \n",
    "# Prettify the plot\n",
    "ax.legend(title=\"Predicted cluster\")\n",
    "ax.grid(linestyle=\"dashdot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbe662-4320-4424-9474-b80ab9ad030c",
   "metadata": {},
   "source": [
    "Compare this to the second plot in the text above, the predicted clusters are very close to the true clusters! Even the two points belonging to the *blue* cluster on the far right in the center have been classified correctly. That's definitely something K-means would not have been able to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ad863-8508-459a-8c55-e5b70c39f064",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️<font color=green>**Question 2**</font>\n",
    "\n",
    "Clustering is part of what we call unsupervised learning. Clusters can occur due to the way the *data generating process* works, but you might never be able to verify which cluster an observation belongs to. That is, in practice, you might never be able to assess how well your clustering works.\n",
    "\n",
    "+ Does this mean unsupervised is less useful than supervised learning? Or more useful?\n",
    "+ Can you think of real life examples where you would like to use unsupervised learning instead of supervised learning?\n",
    "\n",
    "Discuss with your classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cc5d8-c1a8-4caa-9ab1-9d1f7507aeed",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️<font color=green>**Question 3**</font>\n",
    "\n",
    "Create a Gaussian mixture model with 3 components and fit it on the iris data set. \n",
    "\n",
    "1. First, do so by using only the sepal length and sepal width as features.\n",
    "2. Second, do so by using the petal length and petal width **additionally** (i.e., 4 features in total)\n",
    "\n",
    "Plot the predicted clusters in a plot where the $x$-axis is the sepal length and the $y$-axis is the sepal width. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850662f-ef29-4390-be1a-89825abbf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e5629-c02e-4252-bc87-65239a703a8b",
   "metadata": {},
   "source": [
    "#### ➡️ ✏️<font color=green>**Question 4**</font>\n",
    "\n",
    "Repeat the same type of analysis with the crabs dataset. What do you find there? Can you say something about the number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7135d-5839-4be2-b446-618344f9fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entere your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
