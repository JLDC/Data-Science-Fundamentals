{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecac490-abd8-4b7a-9f27-588f6f16a233",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/205-my-own-neural-network-1.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc5941-d630-4819-a118-6cea7cc630fe",
   "metadata": {},
   "source": [
    "# Building our own Neural Network (pt. 1)\n",
    "___\n",
    "In this script we will build our own *home-made* neural network without the help of the `scikit-learn` package's model. We will train our network using **backpropagation** and **gradient descent**, as is customary in neural networks. Backpropagation and gradient descent are closely related, but there is an important difference:  \n",
    "+ **Gradient descent** is the optimization algorithm used to *minimize the loss*.\n",
    "+ **Backpropagation** is the differentiation algorithm used to *compute the gradients*.\n",
    "\n",
    "Going through this notebook will give you a better understanding of what these fancy graphical representations of neural networks really mean in terms of math and programming. For simplicity, we omit any bias terms, i.e., constant-value features in the input or hidden layers.\n",
    "\n",
    "We will look at a classification example with only two input features, a single hidden layer with $n_Z$ hidden nodes, and a single output. The code is flexible with respect to the number of hidden nodes, so you can adjust them to see how the results change with more or less nodes in the hidden layer. When training the network, we will actually use stochastic gradient descent rather than gradient descent. Stochastic gradient descent is a special case of gradient descent. The idea is to compute the gradient using only a random subsample of the data instead of the full dataset. This has three main advantages:\n",
    "1. It is typically quicker to reach a *good* result.\n",
    "2. It is less likely that we end up getting stuck in a local minimum.\n",
    "3. It slightly simplifies the code.\n",
    "\n",
    "As you will see, neural networks entail a lot of matrix multiplications, so hopefully your linear algebra is up-to-date!\n",
    "\n",
    "Note that this way of building a neural network is purely pedagogical. There is no chance that you would actually build your own neural network in a practical scenario. You would simply implement a network using one of the many deep learning packages in Python (see the first notebook *Introduction to Python*, for links to some of these packages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93633358-7ec6-48cc-957c-432784f2f28e",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c3e45-2382-4358-9c2d-34ed226033ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np # Numerical computation package\n",
    "import pandas as pd # Dataframe package\n",
    "import matplotlib.pyplot as plt # Plotting package\n",
    "import matplotlib as mpl # For colormaps\n",
    "np.random.seed(1) # Set the random seed for reproduceability\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a63b2-4f8f-4e4d-a2be-8a9ce32447a8",
   "metadata": {},
   "source": [
    "Once again, will work with the WDBC dataset. There is nothing new here, this is very similar to what we did in the notebook on perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486f975-16e5-44e1-b4ce-c677f8f38805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the WDBC dataset\n",
    "wdbc = pd.read_csv(f\"{DATA_PATH}/wdbc.csv\")\n",
    "# Keep only necessary columns: the diagnosis, the perimeter, and the severity of concave portions\n",
    "# of the cell nucleus\n",
    "wdbc = wdbc[[\"perimeterM\", \"concaveM\", \"diagnosis\"]]\n",
    "wdbc # Display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09fd4d-ce41-4718-9f8e-313369835a47",
   "metadata": {},
   "source": [
    "### üôÄ ü§Ø Standardization\n",
    "\n",
    "First, we will begin by standardizing the data. **This is an absolute must for neural networks**, even when working with a library instead of your own *home-built* network, standardizing your data is key!\n",
    "\n",
    "But why is it so important to standardize with neural networks as opposed to other statistical models? The answer lies in **gradient descent**. In truth, it's not some neural network specific architecture that makes it so important to standardize but simply the application of gradient descent. To see why, consider the following example.\n",
    "\n",
    "Say that we have two features, $\\mathbf{x}_1$ and $\\mathbf{x}_2$, and we are trying to find the weights $w_1$, $w_2$ which minimize the mean squared error $\\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - w_1 x_1^{(i)} - w_2 x_2^{(i)})^2$. So just as we did in some previous notebooks. Recall, that the weight updates are then given by:\n",
    "\n",
    "\\begin{align}\n",
    "w_1 &\\leftarrow w_1 - \\eta \\frac{\\partial \\text{MSE}}{\\partial w_1} \\\\\n",
    "w_2 &\\leftarrow w_2 - \\eta \\frac{\\partial \\text{MSE}}{\\partial w_2} \n",
    "\\end{align}\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "\\begin{align}\n",
    "w_1 &\\leftarrow w_1 - \\eta \\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - w_1 x_1^{(i)} - w_2 x_2^{(i)}) (-x_1^{(i)}) \\\\\n",
    "w_2 &\\leftarrow w_2 - \\eta \\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - w_1 x_1^{(i)} - w_2 x_2^{(i)}) (-x_2^{(i)}).\n",
    "\\end{align}\n",
    "\n",
    "The important part of this equation is the last one. The updates on the weight $w_1$ are **scaled** by the values of the feature $x_1$, and the same happens for the weights $w_2$ with the features $x_2$.\n",
    "\n",
    "Consider the iris example, where $x_1$ was the sepal width in centimeters, and $x_2$ the sepal length in centimeters. Both values in centimeters are on a somewhat similar scale, so this is not a problem. Consider, however, what would happen if instead we were to use the the sepal width in nanometers? Suddenly, our $x_1$ would be on average around 10 million times larger than the $x_2$ and the weight updates of $w_1$ would completely dominate the learning rate! So in the end, we would be dealing with a learning procedure where either $w_1$ has crazy explosive updates or $w_2$ nearly doesn't move, this would make it very hard for our network to converge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6adb0a-4896-471d-8224-7e9449ad61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8343341-29b2-414f-809b-2d7d0306a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features and label data\n",
    "X = np.array(wdbc[[\"perimeterM\", \"concaveM\"]])\n",
    "y = np.array(wdbc[\"diagnosis\"] == \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff1d51-e85e-4b9d-925e-a73da3ec7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413fc1f-87ce-4a78-a1ee-60187ac10bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that our mean and std are (0, 1)\n",
    "print(X.mean(axis=0).round(5))\n",
    "print(X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdc57c-8828-4e95-9e6b-5fea90f2466f",
   "metadata": {},
   "source": [
    "___\n",
    "## Training the neural network\n",
    "\n",
    "There are a few parameters we need to define before we can start with training the network:\n",
    "\n",
    "+ $N_X = K$, the number of nodes in the input layer (number of features we have)\n",
    "+ $N_Z$, the number of nodes in the hidden layer.\n",
    "+ The number of **epochs**. Recall that with gradient descent, we have to specify *how long* we want our procedure to repeat. This is encoded in the number of epochs.\n",
    "+ $\\eta$, the learning rate, a small number which governs *how large* our weight updates will be at each update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9278e-189b-4b6f-bf01-dfaf2e9c0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some parameters\n",
    "N, K = X.shape # Number of observations and features\n",
    "NZ = 3 # Number of hidden nodes\n",
    "epochs = 100 # Number of training epochs\n",
    "eta = 0.01 # Learning rate\n",
    "init_range = [-.5, .5] # The range of our uniform distribution for weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3f2a6-bdcd-49ca-bfdd-4bca29d401ee",
   "metadata": {},
   "source": [
    "___\n",
    "### üôÄ ü§Ø Mathematical formulation\n",
    "\n",
    "Before we go ahead and write the code, it's a good idea to review how a neural network is supposed to work from a mathematical perspective. First, let's go over the individual pieces of our network:  \n",
    "+ $N$, the number of observations ($N=569$) in our case\n",
    "+ $K$, the number of features ($K=2$) in our case\n",
    "+ $N_Z$, the number of nodes in the hidden layer\n",
    "+ $\\mathbf{X}$, a $N \\times K$ matrix of data inputs\n",
    "+ $\\mathbf{y}$, a column vector of targets with $N$ elements\n",
    "+ $\\mathbf{W}_1$, a $K \\times N_Z$ a weight matrix which *passes* the input data to the hidden layer (before activation)\n",
    "+ $g_1: \\mathbb{R} \\to \\mathbb{R}$, an activation function, applied element-wise to the output of the first layer\n",
    "+ $\\mathbf{W}_2$, a $N_Z \\times 1$ a weight matrix (or a column vector) which *passes* the data from hidden layer to the output layer (before activation)\n",
    "+ $g_2: \\mathbb{R} \\to \\mathbb{R}$, an activation function, applied element-wise to the output of the second layer\n",
    "\n",
    "Putting all these elements together, the neural network $f : \\mathbb{R}^{N \\times K} \\to \\mathbb{R}^N$ can be expressed as:\n",
    "\n",
    "$$f(\\mathbf{X}) = g_2(g_1(\\mathbf{X} \\mathbf{W}_1)\\mathbf{W}_2)$$\n",
    "\n",
    "Can you recall what we are training in this neural network? That's right, the weights $\\mathbf{W}_1$ and $\\mathbf{W}_2$. That's all, everything else is *given* or *defined*. Of course, to train our network, we need to start with some values for $\\mathbf{W}_1$ and $\\mathbf{W}_2$. We will do so by randomly initializing each element in $\\text{unif.}[-0.5, 0.5]$.\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5df67-e464-4f59-9339-0db1a16af3f8",
   "metadata": {},
   "source": [
    "#### ü§î Pause and ponder\n",
    "Why is $\\mathbf{W}_1$ a $K \\times N_Z$ matrix and $\\mathbf{W}_2$ a $N_Z \\times 1$ matrix? Could we have chosen other dimensions? \n",
    "\n",
    "If $\\mathbf{A}$ is $P \\times Q$ and $\\mathbf{B}$ is $Q \\times R$, can we compute $\\mathbf{AB}$, and if so, what is its dimension? What about $\\mathbf{BA}$ ?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f3966-9d3b-41bd-a14c-9e988d3f35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weight matrices\n",
    "np.random.seed(72) # Set seed\n",
    "W1 = np.random.rand(K, NZ) # Randomly initialize W1\n",
    "W1 = W1 * (init_range[1] - init_range[0]) + init_range[0] # Constrain to range\n",
    "W2 = np.random.rand(NZ, 1) # Randomly initialize W2\n",
    "W2 = W2 * (init_range[1] - init_range[0]) + init_range[0] # Constrain to range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6215d-52f2-4475-8bd2-5b6ab6847e00",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î Pause and ponder\n",
    "Do you see how the above weights relate to the graphical representation seen in class?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eba08-1491-4418-9518-741755c49202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the sigmoid function as the activation function\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204548e-39ef-42ad-ae74-3408477b12f8",
   "metadata": {},
   "source": [
    "Because we will use it a few times, we define a function for the **forward pass** of our network. Recall the mathematical formulation above, you will see that this function is nothing else but\n",
    "\n",
    "$$f(\\mathbf{X}) = g_2(g_1(\\mathbf{X} \\mathbf{W}_1)\\mathbf{W}_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25c5aa-b212-4caf-bd74-843696093a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass helper\n",
    "def forward_pass(X, W1, W2):\n",
    "    # Compute the matrix multiplication of the input layer\n",
    "    S = X @ W1\n",
    "    \n",
    "    # Pass through the non-linear activation function\n",
    "    Z = sigmoid(S)\n",
    "    \n",
    "    # Compute the matrix multiplication of the hidden layer\n",
    "    T = Z @ W2\n",
    "    \n",
    "    # Pass through the non-linear activation function \n",
    "    # (This should always be sigmoid for the output to be (0, 1))\n",
    "    return sigmoid(T).flatten() # Transform the output back to a vector instead of Nx1 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4640b1-0d0e-48f1-a488-3147d101ca79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a8a5c-9b20-414c-8450-c7696da9e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, we will use to evaluate our predictions\n",
    "# ‚ö†Ô∏è Look at the details of this function only after you have gone through the\n",
    "# ‚ö†Ô∏è code with the training loop below. See the questions at the end of the script\n",
    "def eval_predictions(X, y, W1, W2):\n",
    "    # Use the forward_pass function defined above to compute our estimated probability\n",
    "    prob = forward_pass(X, W1, W2)\n",
    "    \n",
    "    # To avoid log(0), clip the probability to not be exactly zero or one\n",
    "    prob = np.clip(prob, 1e-8, 1-1e-8)\n",
    "    # Calculate the loss function (negative log-likelihood in this case)\n",
    "    loss = - np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "\n",
    "    # Convert probabilities into (0, 1) prediction\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute number of misclassification\n",
    "    misclassifications = np.mean(pred != y)\n",
    "    \n",
    "    # Output results as a dictionary\n",
    "    return {\n",
    "        \"loss\": loss, \n",
    "        \"misclassifications\": misclassifications, \n",
    "        \"prob\": prob, \n",
    "        \"pred\": pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebcbd3-fc92-4517-a9f1-7b38ad286d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializitation of lists for bookkeeping\n",
    "loss_list = []\n",
    "misclassification_list = []\n",
    "\n",
    "# Create an array of indices (which we will shuffle later on) through which we \n",
    "# will iterate. This represent the index of the observations\n",
    "indices = np.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14c8af-3bbf-4cad-8be9-c457f6fcd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this, we use it to print nicely without creating too many lines\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79eedad-78c7-41da-bfa2-d745c5c6655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(72) # Reset random seed for reproduceability\n",
    "# Compute the loss and misclassifications BEFORE training\n",
    "res = eval_predictions(X, y, W1, W2)\n",
    "\n",
    "# Append to our result lists\n",
    "loss_list.append(res[\"loss\"])\n",
    "misclassification_list.append(res[\"misclassifications\"])\n",
    "\n",
    "# Run the full training loop (iterate over the number of training epochs)\n",
    "for epoch in range(epochs):\n",
    "    # Reshuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Iterate through each single data point\n",
    "    for i in indices:\n",
    "        # Note that we use [i:i+1, :] instead of [i, :] to keep it as a 1x2 matrix\n",
    "        Xi = X[i:i+1, :] # Extract features for ith observation,\n",
    "        yi = y[i] # Extract label for ith observation\n",
    "        \n",
    "        # ----- Forward pass -----\n",
    "        # Computes the predictions using Xi, W1, W2. Here we avoid using the\n",
    "        # forward_pass function because we need the hidden nodes values Zi\n",
    "        # for backpropagation. So, instead, we repeat the code (ugh...)\n",
    "        \n",
    "        # Pass to the hidden nodes (pre-activation)\n",
    "        Si = Xi @ W1\n",
    "        # Compute activation function\n",
    "        Zi = sigmoid(Si)\n",
    "        \n",
    "        # Pass to the output nodes\n",
    "        Ti = Zi @ W2\n",
    "        # Compute sigmoid probability transformation\n",
    "        prob_i = sigmoid(Ti)\n",
    "        \n",
    "        # Note that, while we would predict 1 if prob_i ‚â• .5 and 0 otherwise,\n",
    "        # for training, it is better to use the raw probability to update\n",
    "        # the weights in the backward pass. Think of our discussion as to why\n",
    "        # perceptron is not a good learning algorithm!\n",
    "        \n",
    "        # ----- Backward pass -----\n",
    "        \n",
    "        error_i = yi - prob_i\n",
    "        \n",
    "        # Compute the gradient w.r.t. W2. ‚ö†Ô∏è W2 is a vector! See math derivations\n",
    "        grad2_i = -error_i * Zi.T\n",
    "        # Compute the gradient w.r.t. W1. ‚ö†Ô∏è W1 is matrix! Making things even worse\n",
    "        grad1_i = -error_i * Xi.T @ (W2.T * Zi * (1 - Zi))\n",
    "        \n",
    "        # Updating: Move 'eta' units in the direction of the negative gradient\n",
    "        W1 -= eta * grad1_i\n",
    "        W2 -= eta * grad2_i\n",
    "    \n",
    "    # Evaluate the learning process and store the results into our lists\n",
    "    res = eval_predictions(X, y, W1, W2)\n",
    "    loss_list.append(res[\"loss\"])\n",
    "    misclassification_list.append(res[\"misclassifications\"])    \n",
    "        \n",
    "    # Print the current status (üôÄ ü§Ø ignore this part!)\n",
    "    bar = \"\".join([\"#\" if epoch >= t * (epochs // 50) else \" \" for t in range(50)])\n",
    "    stdout.write(f\"\\rEpoch: {epoch+1:>{int(np.floor(np.log10(epochs))+1)}}/{epochs} [{bar}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49a9ff-1f53-471e-8311-5374d45a1dad",
   "metadata": {},
   "source": [
    "___\n",
    "## Evaluating the results\n",
    "That's it, we can go ahead and look at the results of our own custom neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a48ab-1900-474e-a409-0652433840aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# Plot the loss over the epochs (epoch 0 is before training!)\n",
    "axs[0].plot(range(len(loss_list)), loss_list)\n",
    "# Plot the misclassification rate over the epochs\n",
    "axs[1].plot(range(len(misclassification_list)), misclassification_list)\n",
    "# Add title, grid, axis labels\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Epoch number\")\n",
    "axs[0].set_ylabel(\"Loss (negative log-likelihood)\")\n",
    "axs[0].set_title(\"Evolution of loss function over training epochs\")\n",
    "axs[1].set_ylabel(\"Misclassification rate\")\n",
    "axs[1].set_title(\"Evolution of missclassification rate over training epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48423bfd-d2da-4cd0-9ce6-f8dc7162b23c",
   "metadata": {},
   "source": [
    "Pretty neat! Our *home-built* neural network works as expected! It achieves a 90% correct classification rate in approximately 20 epochs and then the performance starts stagnating at around 40 epochs. Of course, as you well know, we shouldn't read too much into this, this is purely in-sample, but independent of the out-of-sample performance, our machinery works, which is the main takeaway.\n",
    "\n",
    "We can also have a look at the weights to see what the final weights are (we use `pandas` for prettier table view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596b16d-2f00-4c82-9ef2-0b3459a5a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the weight matrix from the input layer to the hidden layer\n",
    "pd.DataFrame(W1, columns=[f\"Z{i}\" for i in range(NZ)], index=[\"X1\", \"X2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed45dc-c5fe-4474-99f9-ebabfe3d137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the weight matrix  from the hidden layer to the output layer\n",
    "pd.DataFrame(W2, columns=[\"T\"], index=[f\"Z{i}\" for i in range(NZ)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380de63-98c4-4b09-9c9b-c08e8dcd3a09",
   "metadata": {},
   "source": [
    "___\n",
    "## Visualizing the decision regions\n",
    "\n",
    "Since our feature space is two dimensional, it's easy to *visualize the decision regions* of our neural network. Think about it: every data point can be represented on an $(x_1, x_2)$ plane, and, for every data point, we can make a prediction using our neural network. Combining this, we can create a grid of thinly spaced $(x_1, x_2)$ points, feed them to our neural network and plot them according to the output it suggests.\n",
    "\n",
    "This is nice because it helps us understand the underlying relationship that was uncovered by our neural network. Recall how, during the perceptron class, we drew the perceptron separation line on the features plane. Do you think the neural network will also split the feature space using a line? Why, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ced651-f9fd-41ce-84b9-c73619af54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define granularity and limits of grids\n",
    "npoints = 200\n",
    "x1_lims = (X[:, 0].min() - .1, X[:, 0].max() + .1)\n",
    "x2_lims = (X[:, 1].min() - .1, X[:, 1].max() + .1)\n",
    "\n",
    "# Create the x1 and x2 arrays\n",
    "x1 = np.linspace(*x1_lims, num=npoints)\n",
    "x2 = np.linspace(*x2_lims, num=npoints)\n",
    "preds = np.empty((npoints, npoints))\n",
    "\n",
    "# Compute the predictions of our network for every single combination\n",
    "for i in range(x2.shape[0]):\n",
    "    for j in range(x1.shape[0]):\n",
    "        # Create the 1x2 input\n",
    "        xi = np.array([[x1[j], x2[i]]])\n",
    "        # Compute the prediction of the network\n",
    "        preds[i, j] = forward_pass(xi, W1, W2)\n",
    "\n",
    "# Small trick to scale back values to their value before standardization\n",
    "Xt = scaler.inverse_transform(np.array([x1, x2]).T)\n",
    "x1, x2 = Xt[:, 0], Xt[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c38ce-4dc6-4313-8c9f-1d4fabcdca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the canvas\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Add decision regions\n",
    "ax.contourf(x1, x2, preds, cmap=mpl.colormaps[\"coolwarm\"], alpha=0.8)\n",
    "\n",
    "# Add true data points\n",
    "for diag in [\"B\", \"M\"]:\n",
    "    subset = wdbc.loc[wdbc[\"diagnosis\"] == diag]\n",
    "    ax.scatter(subset[\"perimeterM\"], subset[\"concaveM\"], label=diag, alpha=0.9)\n",
    "    \n",
    "# Add grid, labels, etc.\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Perimeter\")\n",
    "ax.set_ylabel(\"Concavity\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc233f95-0c81-4d3d-81a0-7eddc2679ec6",
   "metadata": {},
   "source": [
    "The benign tumors are shown in blue above and the malignant ones in orange. The color in the background are the decision regions of our neural network. We notice that the decision boundary is a straight line. The intensity of the color regions show *how sure* the neural network is in making the prediction, i.e. in the dark purple area in the top right, the network assigns a very high probability that the tumor is malignant while in the lower left, the probability is very low. Around the white band however, the probability is around 0.5. Of course, in the end, a tumor is always classified as either benign or malignant, but this helps us visualize where the predictions are more *unstable*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcb81b-1698-474c-bf95-593dafb8fa16",
   "metadata": {},
   "source": [
    "___\n",
    "### Exercises\n",
    "\n",
    "#### <font style=\"color:green\">**‚û°Ô∏è ‚úèÔ∏è Question 1**</font>\n",
    "Try playing around with the parameters. Change the hidden nodes, learning rate, number of epochs.\n",
    "\n",
    "How do the results change? Do they get better or worse? How do the decision region change?\n",
    "\n",
    "*Hint*: If you encounter an error, try changing some parameters back. A too high learning rate can cause your weights to explode and `numpy` won't be able to handle such large numbers (in particular when trying to exponentiate them!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abad4c-311b-4b1b-8e90-67db6b3d586c",
   "metadata": {},
   "source": [
    "#### <font style=\"color:green\">**‚û°Ô∏è ‚úèÔ∏è Question 2**</font>\n",
    "\n",
    "Carefully inspect the code of `eval_predictions(...)` and `forward_pass(...)`.\n",
    "\n",
    "+ Why do we compute the `forward_pass` manually again in the training loop? Is there a difference?\n",
    "+ Do we need `eval_predictions` to train our network?\n",
    "\n",
    "Discuss with your classmates and try to explain the code to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992dc9ca-a9ab-43dd-9b3d-f8bf86f870c6",
   "metadata": {},
   "source": [
    "#### <font style=\"color:green\">**‚û°Ô∏è ‚úèÔ∏è Question 3 (optional)**</font>\n",
    "\n",
    "Read the *advanced* part of the class notes on neural networks and try to implement the contents into the code. \n",
    "\n",
    "*Hint*: The main difference is that there are two separate *models* for each of the two output layers. This means that we treat the output layer in the same way as we would if we had more than 2 output layers, using cross-entropy as the loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
