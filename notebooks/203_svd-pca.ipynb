{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016475b8-6817-4cf7-af7f-ff8520d7068d",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/203_svd-pca.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f853f-0d12-4100-8f96-aa7f0258e7c7",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition and Principal Component Analysis\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fae3d1-87b6-470c-a69d-193867fde84c",
   "metadata": {},
   "source": [
    "In this notebook, we will look at two significant linear algebra concepts: singular value decomposition (SVD) and principal component analysis (PCA). SVD and PCA are essential in machine learning, especially when we want to extract crucial information from our data or/and reduce dimensionality.\n",
    "\n",
    "There is a decent amount of mathematical theory behind both concepts, but this should not scare you! While there will be a fair bit of maths in this notebook. It's okay if you need help understanding everything on your first readthrough. As always, it's key to understand the intuition behind our tools and why and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c18016-3467-4f72-b95d-69364f7bf8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "\n",
    "### üôÄ ü§Ø Mathematical Formulation\n",
    "\n",
    "For a real $m \\times n$ matrix $\\mathbf{A}$, the SVD is an algorithm which factorizes the matrix $\\mathbf{A}$ into three matrices:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U \\Sigma V}^\\top,$$\n",
    "\n",
    "where\n",
    "+ $\\mathbf{U}$ is an $m \\times m$ [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) (i.e., $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{UU}^\\top = \\mathbf{I}$), \n",
    "+ $\\mathbf{\\Sigma}$ is an $m \\times n$ (rectangular) [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix) (i.e., only its diagonal values can be non-zero, everything outside of the diagonal is zero), and \n",
    "+ $\\mathbf{V}$ is an $n \\times n$ [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) (i.e., $\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{VV}^\\top = \\mathbf{I}$).\n",
    "\n",
    "Writing it out gives:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\underset{m\\times n}{\\overbrace{\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}  & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22}  & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m1} & a_{m2}  & \\dots & a_{mn} \\\\\n",
    "\\end{bmatrix}}^{\\mathbf{A}}} =\n",
    "\\underset{m\\times m}{\\overbrace{\\begin{bmatrix}\n",
    "u_{11}  & \\dots  & u_{1m} \\\\\n",
    "\\vdots &  \\ddots & \\vdots \\\\\n",
    "u_{m1} & \\dots & u_{mm}\n",
    "\\end{bmatrix}}^{\\mathbf{U}}} \\\n",
    "\\underset{m\\times n}{\\overbrace{\\begin{bmatrix}\n",
    "\\sigma_1 & 0  & \\dots & 0 \\\\\n",
    "0 & \\sigma_2  & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0  & \\dots & 0 \\\\\n",
    "\\end{bmatrix}}^{\\mathbf{\\Sigma}}} \\\n",
    "\\underset{n\\times n}{\\overbrace{\\begin{bmatrix}\n",
    "u_{11}  & \\dots  & u_{1n} \\\\\n",
    "\\vdots &  \\ddots & \\vdots \\\\\n",
    "u_{n1} & \\dots & u_{nn}\n",
    "\\end{bmatrix}}^{\\mathbf{V}^\\top}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Beware that $\\mathbf{\\Sigma}$ has $\\min(m, n)$ non-zero values only, i.e., there will also be zeros on the diagonal if $\\mathbf{A}$ is not a square matrix (i.e., if $m \\neq n$).\n",
    "___ \n",
    "\n",
    "Okay, let's reiterate with a bit less math: **SVD decomposes a matrix into three matrices.**\n",
    "+ There is one orthogonal matrix which has as many rows and columns as $\\mathbf{A}$ has **rows**.\n",
    "+ There is one orthogonal matrix which has as many rows and columns as $\\mathbf{A}$ has **columns**.\n",
    "+ There is a matrix which is zero everywhere outside of the diagonal. This matrix has the same dimension as $\\mathbf{A}$.\n",
    "\n",
    "\n",
    "[Wikipedia proposes a very nice illustration of SVD](https://upload.wikimedia.org/wikipedia/commons/c/c8/Singular_value_decomposition_visualisation.svg).\n",
    "\n",
    "\n",
    "We call the non-zero elements of $\\mathbf{\\Sigma}$ the **singular values**. The columns of $\\mathbf{U}$ and $\\mathbf{V}$ are called the **left-singular** and **right-singular vectors** respectively.\n",
    "\n",
    "It is interesting to note that **the SVD always exists!** While many other matrix factorization do not work for any real matrix, the SVD does. In any case, this might be a lot to process, let's look at SVD in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13601a1-5713-4738-882f-c9775868f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import svd # Import svd directly to make our code shorter\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8571d2-ded0-415b-b936-5e081d497bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for replicability\n",
    "np.random.seed(72)\n",
    "\n",
    "# Define the number of rows and columns\n",
    "m, n = 4, 12\n",
    "\n",
    "# Create a random m x n matrix\n",
    "A = np.random.rand(m, n) \n",
    "\n",
    "# Compute its singular value decomposition\n",
    "U, S, V = svd(A)\n",
    "\n",
    "# Print the shape of the matrices\n",
    "print(f\"A has shape {A.shape}\")\n",
    "print(f\"U has shape {U.shape}\")\n",
    "print(f\"S has shape {S.shape}\")\n",
    "print(f\"V has shape {V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6526d-27a6-4064-9330-7ead0a45a568",
   "metadata": {},
   "source": [
    "Looking at the shapes, we notice that `S` is a vector and not a diagonal matrix like we initially claimed. This is the way most programming languages do SVD. Fortunately, it's very easy to get our $m \\times n$ diagonal matrix from this.\n",
    "\n",
    "However, since $\\mathbf{\\Sigma}$ has some zero element on the diagonal, we can also ignore these elements and drop the unnecessary columns of $\\mathbf{U}$ or rows of $\\mathbf{V}$. To do this, we simply pass the parameter `full_matrices=False` to our `svd` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c3663-277a-426d-bb5e-9a84d427034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SVD in reduced-form\n",
    "U, S, V = svd(A, full_matrices=False)\n",
    "\n",
    "# Print the shape of the matrices\n",
    "print(f\"A has shape {A.shape}\")\n",
    "print(f\"U has shape {U.shape}\")\n",
    "print(f\"S has shape {S.shape}\")\n",
    "print(f\"V has shape {V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e18ef4-b916-4068-b733-dcdee40818af",
   "metadata": {},
   "source": [
    "Hence, by using this reduced form SVD, we have dropped 8 rows on our $\\mathbf{V}^\\top$ matrix. Finally, we still need to transform our vector of singular values `S` to a diagonal matrix, this can easily be done using `np.diag`.\n",
    "\n",
    "So, if SVD really does work, we should be able to obtain `A` by multiplying `U`, `np.diag(S)`, and `V` together... let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75f7fb-e564-4cce-84f2-24756427fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reconstructed = U @ np.diag(S) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368bd25-7847-4ba7-aa8b-485027b11d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that A and A_reconstructed are the same \n",
    "# (up to some small numerical error)\n",
    "np.all(np.abs(A - A_reconstructed) <= 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e6a84-2819-44e6-9683-c764e759aada",
   "metadata": {},
   "source": [
    "Great, we got SVD to work! ... but what shall we make of it? Why is such a factorization even useful? Are we just doing linear algebra for fun? ü§®\n",
    "\n",
    "Unfortunately, SVD is somewhat of an acquired taste. At first, it seems like plain linear algebra, but as your knowledge about machine learning grows, you will start seeing SVD in many places, even some you would not expect. SVD is a key component in signal processing, image processing, big data and many more domains!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d37c84-e85b-4a43-8f30-9481f5930ed9",
   "metadata": {},
   "source": [
    "### Image Compression\n",
    "\n",
    "Let us look at SVD in the context of image processing, in particular image compression. As you already know, you can think of an image as a matrix: **A grayscale image with a height of $m$ pixels and a width of $n$ pixels can be represented by an $m \\times n$ matrix, where each cell is the intensity of the pixel.** Different programming languages use different conventions, in Python, we will use *integer* values in $[0, 255]$ to represent the intensity. $0$ is completely black, and $255$ is completely white. Of course, if you have a colored image, e.g., using RGB (red-green-blue) colors, you can split this into three matrices: one for the red color intensity, one for the green color intensity and one for the blue color intensity.\n",
    "\n",
    "A well-known package called Pillow allows us to work with images in Python. We can load them, display them, and transform them to numpy arrays (we can also do many more things, but that's what we will be doing for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219f9df-eebc-474a-8d3e-0178f869511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillow is a package that allows us to work with images\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe00e7e-8c1e-4c18-85eb-1d746a8f69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image using PIL\n",
    "img = Image.open(f\"{DATA_PATH}/images/gauss.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70817473-c926-4b73-8b09-1456fb4abd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.resize((300, 400)) # Show the image (resized to a smaller size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f12d6-6838-4a94-9a2a-6ab4a81659f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the image to a numpy array\n",
    "img_arr = np.asarray(img)\n",
    "# Show the shape of the array\n",
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3aeeb2-a7d0-449c-9b44-8f251a6019b9",
   "metadata": {},
   "source": [
    "As expected, the dimensions of the image array in numpy refer to `(height, width, number of colors)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f01270-b4d6-4a60-bb11-d88e4b6affdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 rows and 10 columns of the \"red\" layer of the above image\n",
    "img_arr[:10, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aabbe0-0ace-4334-99e6-4e5991f8ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale by taking the mean of all three color layers\n",
    "# notice that Python expects to have an array of integers between [0, 255], \n",
    "# hence we need to transform back to integer!\n",
    "def img_to_grayscale(img):\n",
    "    # Only transform to grayscale if it has multiple layers in the 3rd dimension\n",
    "    if len(img.shape) > 2:\n",
    "        return img.mean(axis=2).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a13c18-113c-4982-8734-5a90de918598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the image to grayscale\n",
    "img_gray = img_to_grayscale(img_arr)\n",
    "\n",
    "# Read the image using Pillow and display it again\n",
    "Image.fromarray(img_gray).resize((300, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051d6d-7882-4f1e-9ac3-9aa8656d2044",
   "metadata": {},
   "source": [
    "Now that we better understand how to load an image, convert it to a numpy array, and convert back a numpy array to an image, all we have to is to add a step where we perform linear algebra on our numpy array, so let's get back to our SVD.\n",
    "\n",
    "SVD is particularly helpful to **approximate a matrix using a lower rank** (recall, the [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) is the number of linearly independent columns in a matrix).\n",
    "\n",
    "Let's say that we want to approximate the image above by using a rank of $k$. In this case, we can use **truncated SVD**, which means that we will reconstruct our initial matrix by using only some parts of the $\\mathbf{U}$, $\\mathbf{\\Sigma}$, and $\\mathbf{V}^\\top$ matrices. To be more precise, the truncated SVD of rank $k$ is:\n",
    "\n",
    "$$\\tilde{\\mathbf{A}}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k \\mathbf{V}^\\top_k,$$\n",
    "\n",
    "where:\n",
    "+ $\\mathbf{U}_k$ is the $m \\times k$ matrix formed by the first $k$ columns of $\\mathbf{U}$\n",
    "+ $\\mathbf{\\Sigma}_k$ is the diagonal matrix of the first $k$ singular values, and\n",
    "+ $\\mathbf{V}_k^\\top$ is the $k \\times n$ matrix formed by the first $k$ rows of $\\mathbf{V}^\\top$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43769a3-9b85-43b2-bfa7-e3b087dd9639",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è <font style=\"color: green\">**Question 1**</font>\n",
    "\n",
    "Write a function, called `truncated_svd(A, k)`, which takes as input a matrix `A` and a rank `k`. This function should\n",
    "\n",
    "1. Perform `svd` using numpy on the matrix `A` (use `full_matrices=False`)\n",
    "2. Return the matrices $\\mathbf{U}_k$, $\\mathbf{\\Sigma}_k$ and $\\mathbf{V}^\\top_k$. This means you have to subset the matrices correctly (if you do not remember how to subset matrices, go back to notebook 01d, however, it is very close to subsetting in pandas!), and you also want to directly turn the vector of singular values into a diagonal matrix (since `svd` will give you a vector of singular values instead of the diagonal matrix!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ea117-e5e5-4f42-b22d-482fb3715fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "def truncated_svd(A, k):\n",
    "    pass # ‚¨ÖÔ∏è ... replace this with your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c759785-0367-4b76-a81d-6cc5dfa72f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your function seems good, run this code to check if it works\n",
    "#check_truncated_svd(truncated_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec1f9d-d440-49c9-88ff-13196a72a737",
   "metadata": {},
   "source": [
    "Let's recap. We claimed that, using truncated SVD, we can approximate our initial matrix by a lower rank matrix, but how good will this approximation be? Well let's look at a visual example. In the code below, $k$ is the rank of our approximation, the higher the $k$, the better the approximation will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e8b42-8cc7-41dd-a99c-cf42fb783752",
   "metadata": {},
   "source": [
    "We will also define a useful function below that allows us to rescale the values of our data in the range $[0, 255]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe22ee-fbd9-4093-a3ad-4e5bfbd48a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale an array such that its values are within [0, 255]\n",
    "rescale = lambda x: (x - x.min()) / (x.max() - x.min()) * 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ada5a-1d1f-4d86-9f8c-b6a2cfbc69ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è <font style=\"color: green\">**Question 2**</font>\n",
    "\n",
    "Plot singular values for all the three color representations. What does it tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd86b3f-1c94-45a8-a206-ded965255f06",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è <font style=\"color: green\">**Question 3**</font>\n",
    "\n",
    "Run the code below, change the value of `k`, e.g., try out `k = 2`, `k = 5`, `k = 10`, `k = 20`, `k = 30`, `k = 50`, `k = 100`. Of course, you can also select another image from the images folder!\n",
    "\n",
    "\n",
    "What do you observe? How is what you see related to Question 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffe20f-5bb0-4f2e-89e7-3ead70133e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rank of our approximation\n",
    "k = 1\n",
    "\n",
    "# Select the image\n",
    "img_path = f\"{DATA_PATH}/images/gauss.jpg\"\n",
    "img = np.asarray(Image.open(img_path))\n",
    "\n",
    "# Perform truncated SVD\n",
    "U, S, V = truncated_svd(img_to_grayscale(img), k)\n",
    "\n",
    "# Reconstruct the image and display it using Pillow\n",
    "Image.fromarray(rescale(U @ S @ V).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3db137-c6be-4366-9cd7-abee9e328bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, SVD also works with colors, we just have to apply SVD to every\n",
    "# one of the three matrices for red, green, and blue, let's write two functions\n",
    "# to make things easier\n",
    "\n",
    "# Recompose a matrix given the U, S, and V matrices\n",
    "recompose = lambda U, S, V: rescale(U @ S @ V).astype(np.uint8)\n",
    "# Do truncated SVD with rank k and recompose the matrix\n",
    "svd_recompose = lambda A, k: recompose(*truncated_svd(A, k))\n",
    "\n",
    "# The rank of our approximation\n",
    "k = 1\n",
    "\n",
    "# Select the image and pass it to a numpy array\n",
    "img_path = f\"{DATA_PATH}/images/ml_linalg.png\"\n",
    "img = np.asarray(Image.open(img_path))\n",
    "\n",
    "# Perform truncated SVD and recomposition on each axis \n",
    "# üôÄ ü§Ø the min(3, img.shape[2]) is because sometimes, images have 4 \n",
    "# \"color\" matrices: R, G, B, and transparency\n",
    "# We want to ignore transparency for this example\n",
    "img_approximated = np.dstack(\n",
    "    [svd_recompose(img[:, :, i], k=k) for i in range(min(3, img.shape[2]))]\n",
    ")\n",
    "\n",
    "# Reconstruct the image and display it using Pillow\n",
    "Image.fromarray(img_approximated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cc3c9-acf8-451e-8b63-cc78a099970c",
   "metadata": {},
   "source": [
    "### How to decide better about number k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9909f2-47d5-4cc6-95c6-bef505437b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Variance explained by each singular vector\n",
    "# Select the image\n",
    "img_path = f\"{DATA_PATH}/images/gauss.jpg\"\n",
    "img = np.asarray(Image.open(img_path))\n",
    "# scale the image matrix befor SVD\n",
    "img_mat_scaled= (img-img.mean())/img.std()\n",
    "# Perform  SVD\n",
    "# Compute the SVD and plot for one color\n",
    "U, S, V = svd(img_mat_scaled[:,:,0], full_matrices=True)\n",
    "var_explained = np.round(S**2/np.sum(S**2), decimals=3)\n",
    "\n",
    "# Variance explained top Singular vectors\n",
    "var_explained[0:50]\n",
    "\n",
    "# load seaborn for plotting\n",
    "# Plot the singular values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(var_explained[0:50], color=\"dodgerblue\")\n",
    "plt.xlabel('Singular Vector', fontsize=16)\n",
    "plt.ylabel('Variance Explained', fontsize=16)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Line_Plot_with_Pandas_Python.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a7c3b-2e53-4a46-8732-00486f16096b",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è <font style=\"color: green\">**Question 4**</font>\n",
    "\n",
    "Can you now decide better what $k$ to take? Analyse your previous results and determine how many singular values to keep for each color so that the mean explained variance is 0.98. Reconstruct the image using that number $k$. What are your conclusions? What is the compression ratio we achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230bae4-f55e-45dd-b7a4-d1c8782e2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We provide solution only for 1 color, the rest compute yourself\n",
    "#np.sum(var_explained[:21])\n",
    "k = 21\n",
    "# Select the image\n",
    "img_path = f\"{DATA_PATH}/images/gauss.jpg\"\n",
    "img = np.asarray(Image.open(img_path))\n",
    "\n",
    "# Perform truncated SVD\n",
    "U, S, V = truncated_svd(img_to_grayscale(img), k)\n",
    "\n",
    "# Reconstruct the image and display it using Pillow\n",
    "Image.fromarray(rescale(U @ S @ V).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74dfa9d-a35b-48ec-9a8e-742cfdb4d23e",
   "metadata": {},
   "source": [
    "___\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "We have now learned a bit about SVD and we have seen how it can be used to approximate matrices. In this context, images a particularly insightful. As humans, visualizing how increasing the rank $k$ of our approximation yields a clearer image is easier to understand than simply looking at numbers in a matrix.\n",
    "\n",
    "In a very non-formal way, using SVD, we find that we don't need all the information in the original image to get a clear picture. Indeed, if you play around with the $k$ in the examples above, you will not be able see a big difference between an approximation with a relatively high $k$ and the original image. \n",
    "\n",
    "Furthermore, when our chosen approximation rank $k$ is low, an increase in one unit of $k$ will yield a much bigger improvement than if it is already large. For instance, try going from $k=1$ to $k=2$ and then try going from $k=100$ to $k=101$, you will probably not notice any difference in the latter case.\n",
    "\n",
    "Pushing this idea further, we can imagine that not all information is equal. When dealing with a massive data set, it could clearly be useful to somehow find a way to summarize the information it contains.\n",
    "\n",
    "Karl Pearson proposed principal component analysis (PCA) in 1901 as a technique to **explain variance in the data**. Once again, PCA is relatively heavy on linear algebra but the main point for you is to get a good intuition as to why and when PCA can be useful.\n",
    "\n",
    "Before diving into the idea of PCA, it is good to revise the concept of an [orthonormal basis](https://en.wikipedia.org/wiki/Orthonormal_basis). Without going to deep into the mathematics, an orthonormal basis is a set of unit vectors (i.e., vectors with length one) that are orthogonal to each other (i.e., their inner product is zero). The simplest orthonormal basis we can think of is defined by the basis vectors $[1, 0]$ and $[0, 1]$, it is also the standard basis in $\\mathbb{R}^2$.\n",
    "\n",
    "The main goal behind PCA is to create an orthonormal basis with basis vectors such that:\n",
    "1. the data projected along the first basis vector (PC1) has the **maximum variance**\n",
    "2. the data projected along the second basis vector (PC2) has the maximum variance given that it is orthogonal to the first basis vector.\n",
    "3. the data projected along the $n^\\text{th}$ basis vector has the maximum variance given that it is orthogonal to the $(n-1)^\\text{th}$ basis vector.\n",
    "\n",
    "The basis vectors defined by PCA are also called the **principal components**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e37b74-2bc3-473f-b2d5-30bae545e6b2",
   "metadata": {},
   "source": [
    "___\n",
    "### Visualizing PCA\n",
    "\n",
    "Thinking about orthonormal bases might seem somewhat abstract and unnecessarily complicated. Luckily, when working in two dimensions, it is easy to visualize linear algebra ideas, so let's try making it clearer with some plots...\n",
    "\n",
    "Consider that you have the following data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72f260-a46b-45a1-9116-c410bf9499b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "N = 500 # Number of data points\n",
    "X1 = 0.75 * np.random.randn(N,1) # Generate N random X1 values\n",
    "X2 = .75 * X1 + np.random.randn(N,1) * 5e-1 # Generate X2 values based on X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e40645-0d7d-42c3-a6f2-63b4c99d4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add data points\n",
    "ax.scatter(X1, X2, alpha=0.8)\n",
    "\n",
    "# Add standard basis vectors\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.1, width=0.02, color=\"black\", label=\"Standard basis vectors\")\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.1, width=0.02, color=\"black\")\n",
    "\n",
    "# Beautify plot\n",
    "ax.set_xlabel(\"X1\")\n",
    "ax.set_ylabel(\"X2\")\n",
    "ax.set_xlim((-3, 3))\n",
    "ax.set_ylim((-3, 3))\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6c2a5-c5b0-4444-91ed-69b2155efdd7",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 5**</font>\n",
    "If you had to draw a line in this plot such that the points along this line have the largest variance, how would this line look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170e46e-2d8d-4888-8182-54a0ecd769ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6da21-f2e2-4f29-8b90-57fab61834b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA with 2 components on the data points\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(np.hstack((X1, X2))) # Fit PCA on the data\n",
    "c = pca.components_ # Extract components to make later code more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abbde5-43f4-4d5c-aaf8-c53fe79386d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üôÄ ü§Ø There is no need to focus too much on this code, the important part is \n",
    "# the resulting plot!\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add data points\n",
    "ax.scatter(X1, X2, alpha=0.8)\n",
    "\n",
    "# Add standard basis vectors\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.1, width=0.02, color=\"black\", \n",
    "         label=\"Standard basis vectors\")\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.1, width=0.02, color=\"black\")\n",
    "\n",
    "# Add PCA basis vectors\n",
    "ax.arrow(0, 0, c[0][0], c[0][1], head_width=0.1, width=0.02, color=\"red\", \n",
    "         label=\"PCA basis vectors\")\n",
    "ax.arrow(0, 0, c[1][0], c[1][1], head_width=0.1, width=0.02, color=\"red\")\n",
    "\n",
    "# Beautify plot\n",
    "ax.set_xlabel(\"X1\")\n",
    "ax.set_ylabel(\"X2\")\n",
    "ax.set_xlim((-3, 3))\n",
    "ax.set_ylim((-3, 3))\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2331842-9f1e-4092-b356-9dc8902876df",
   "metadata": {},
   "source": [
    "Perhaps you answered the question above by saying that the line which explains the most version should be the diagonal from the bottom left to the top right of the plot. If that is the case, you are right. The above plot shows us the basis vectors of the orthonormal basis defined by PCA in red. As we can see from the data points, the data varies the most along those vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45168e55-5ad0-40e1-ad9b-397a97b66ab0",
   "metadata": {},
   "source": [
    "Consider our randomly generated data and the basis vectors of the orthonormal basis of the PCA. We can transform our data to this new basis, as the animation below shows. Notice how, **after the change of basis, the first principal component (that which explains the most variance) is on the horizontal axis, and, the second principal component is on our vertical axis**. Of course, all of this generalizes to more than two dimensions, but it becomes more difficult to grasp the intuition visually!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6337c-0c7f-4d09-a0eb-d68efc5c66ef",
   "metadata": {},
   "source": [
    "![test](https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data/images/pca.gif)\n",
    "\n",
    "Notice how, as we transform the data space, this brings the PCA basis vectors in place of our standard basis vectors, this is a visualization of a **change of basis**.\n",
    "\n",
    "‚ö†Ô∏è However, be careful with *reading too much into visualizations*. In particular, the above plot shows how the change of basis due to PCA is akin to a *rotation* of the data. This is not always the case. Without getting into too many details, the change of basis is a matrix multiplication, i.e., it can be any linear transformation. If you are not too sure of what a change of basis is or how to interpret a matrix multiplication, we highly recommend looking at 3Blue1Brown's amazing playlist: [The Essence of Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra). It is without a doubt one of the most intuitive explanations of linear algebra basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e35dba-5e17-4b42-bb74-9a849342a926",
   "metadata": {},
   "source": [
    "___\n",
    "## Exploratory Data Analysis\n",
    "Once again, we have to ask ourselves: **buy why is this useful?**. Given the basis vectors of the PCA, we can multiply single data points by these vectors, which gives us the **principal components of each data point**. \n",
    "\n",
    "Do you remember when we learned about overfitting and we saw that we could add polynomials of our features to enrich our data and obtain a better fit? In a sense, we can also view principal components as potential new features, in fact, **the features which explain the most variance in the data!**\n",
    "\n",
    "PCA is particularly useful in **exploratory data analysis**, it can give us insights on whether the **variation** in the data explains something. We might want to use it as a precursor for a more formal data analysis or to tease out some meaningful differences in the data. Let's have a look at how PCA can be useful using our old friend, the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae95f-e432-47d9-be15-2587ae03f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot helper. No need to focus on this code! This is only useful to make the correlation plots\n",
    "# you see below. Focus on the plots, not the code to create them.\n",
    "def corrplot(df, features, color_features, shape_features=None, figsize=(12, 12)):\n",
    "    markers = [\"o\", \"^\", \"s\", \"P\", \"*\", \"H\", \"X\"]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    p = len(features)\n",
    "    # Create the canvas\n",
    "    fig, axs = plt.subplots(p, p, figsize=figsize)\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            ax = axs[i, j]\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "            if i == j:\n",
    "                ax.annotate(features[i], (0.5, 0.5), ha=\"center\", va=\"center\")    \n",
    "            else:\n",
    "                #ax.grid(linestyle=\"dashdot\")\n",
    "                for c,f in enumerate(df[color_features].unique()):\n",
    "                    if shape_features:\n",
    "                        for k,m in enumerate(df[shape_features].unique()):\n",
    "                            dft = df[(df[color_features] == f) & (df[shape_features] == m)]\n",
    "                            if i == 0 and j == 1:\n",
    "                                ax.scatter(dft[features[i]], dft[features[j]], label=f\"{f} ({m})\",\n",
    "                                           alpha=0.8, marker=markers[k], color=colors[c])\n",
    "                            else:\n",
    "                                ax.scatter(dft[features[i]], dft[features[j]], marker=markers[k], alpha=0.8, color=colors[c])\n",
    "                    else:\n",
    "                        dft = df[df[color_features] == f]\n",
    "                        if i == 0 and j == 1:\n",
    "                            ax.scatter(dft[features[i]], dft[features[j]], label=f, alpha=0.8)\n",
    "                        else:\n",
    "                            ax.scatter(dft[features[i]], dft[features[j]], alpha=0.8)\n",
    "                if i == 0: #\n",
    "                    if j % 2 == 1:\n",
    "                        ax.xaxis.set_visible(True)\n",
    "                        ax.xaxis.tick_top()\n",
    "                elif i == p - 1:\n",
    "                    if j % 2 == 0:\n",
    "                        ax.xaxis.set_visible(True)\n",
    "                ###\n",
    "                if j == 0:\n",
    "                    if i % 2 == 1:\n",
    "                        ax.yaxis.set_visible(True)\n",
    "                elif j == p - 1:\n",
    "                    if i % 2 == 0:\n",
    "                        ax.yaxis.set_visible(True)\n",
    "                        ax.yaxis.tick_right()\n",
    "    fig.legend()\n",
    "    fig.suptitle(\"Correlation Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081ccd4-a9fd-46c1-a094-53a185317ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris = pd.read_csv(f\"{DATA_PATH}/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb2b6b-0e23-4d1c-8832-98e5881dbdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PCA, we want to standardize our data, i.e., de-mean it and divide it by the standard deviation\n",
    "standardize = lambda x: (x - x.mean()) / x.std()\n",
    "features = iris.columns[:-1] # All sepal/petal length/width\n",
    "iris[features] = standardize(iris[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98e881-1362-460b-957b-bdeae61b92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for our principal components\n",
    "principal_components = [f\"PC{i+1}\" for i in range(len(features))]\n",
    "\n",
    "# Run PCA with 4 principal components on our iris data\n",
    "pca = PCA(n_components=4)\n",
    "iris[principal_components] = pca.fit_transform(iris[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9316b-6e92-4a46-a028-688406f767c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a correlation plot of the (standardized) features\n",
    "corrplot(iris, features, \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1fb07-bf87-460c-9ace-7e20880a0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a correlation plot of the principal components\n",
    "corrplot(iris, principal_components, \"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6da8f9-b465-4185-ae3e-1e9537b5606e",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 6**</font>\n",
    "Compare the principal component correlation plot with the one based on the *true* features. Discuss it with your classmates: \n",
    "+ What do you observe?\n",
    "+ How do you feel about the first principal component **PC1** compared to the further principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800cd76-75fc-4cfe-b099-7a8d9d55bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs = pd.read_csv(f\"{DATA_PATH}/crabs.csv\")\n",
    "crabs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec619d-c753-44e2-9a51-8ba412631bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"FL\", \"RW\", \"CL\", \"CW\", \"BD\"]\n",
    "crabs[features] = standardize(crabs[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ebb50-e5a9-4d21-b200-15fe8bbfa8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for our principal components\n",
    "principal_components = [f\"PC{i+1}\" for i in range(len(features))]\n",
    "\n",
    "# Run PCA with 5 principal components on our crabs data\n",
    "pca = PCA(n_components=5)\n",
    "crabs[principal_components] = pca.fit_transform(crabs[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4cebfc-44f0-474f-89f9-7a6e200441fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a correlation plot of the (standardized) features)\n",
    "corrplot(crabs, features, \"species\", \"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487854fd-45e0-4e72-a9dd-9f7a140a3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a correlation plot of the principal components\n",
    "corrplot(crabs, principal_components, \"species\", \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da406588-b44c-4228-ae40-7c21dcce051b",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 7**</font>\n",
    "Compare the principal component correlation plot with the one based on the *true* features. Discuss it with your classmates: \n",
    "+ What do you observe?\n",
    "+ What has changed between this plot and the one we did for the iris data set?\n",
    "+ For which of the two data sets do you think PCA makes more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5add56-6f7e-462b-9158-eb264ff59cd5",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 8**</font>\n",
    "Use the same method of decision the number of components as in the SVD image compression. Use the iris and crabs data and provide your analysis.\n",
    "Run dimensionality reduction and reconstruction of Gauss picture with PCA.\n",
    "How do your results compare to the ones in the SVD part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a87ad-30c3-4f61-b560-2930a4246d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
