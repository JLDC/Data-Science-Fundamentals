{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cec5a7-3bce-4512-b7d2-ccb98e2a3faa",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/101_iris-perceptron.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c7041-2017-45ea-a91f-51238ef71b96",
   "metadata": {},
   "source": [
    "# Perceptron Learning for Iris Data\n",
    "___\n",
    "\n",
    "In this notebook, we will implement the same learning algorithm for the **iris** dataset that you have implemented in Excel during the first session. It is important to note that the way we implement the algorithm in this notebook is neither efficient, nor is it the way you would truly implement a neural network in practice (indeed, a perceptron model is a special case of a neural network). This example is set up to be pedagogical, such that you better understand the mechanism underlying the perceptron.\n",
    "\n",
    "### üßë‚Äçüíª <font color=green>**Your Task**</font>\n",
    "\n",
    "Please go through the below code and solve the <font color=green>**Questions**</font> contained in the notebook (indicated with a green heading). You will also need the Excel implementation of the iris learning engine from our first class. You find a clean Excel solution on Canvas under Files > Data. It may also help to look again at the instructions for the Excel task in the slides for the first class.\n",
    "\n",
    "**It's probably most productive if you work in small groups.** If you get stuck, please let us know and we drop by. We are there to support your learning experience. Sitting on your desk in the status stuck for a longer time is not very productive. \n",
    "\n",
    "Please note your answers to the questions on a (digital) piece of paper or directly code/write it into code/markdown cells below (depending whether it is a thinking or coding question). We will discuss the solutions in class. To start the discussion, we may randomly call some of you to share your thoughts and solution ideas :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9205c-fc63-4826-a380-cd83633cda9f",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9558ca-3e86-40c1-8e72-cf9bd82b0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np # Numerical computation package\n",
    "import pandas as pd # Dataframe package\n",
    "import matplotlib.pyplot as plt # Plotting package\n",
    "np.random.seed(1) # Set the random seed for reproduceability\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed84192-a113-4746-842f-f0c12496dfca",
   "metadata": {},
   "source": [
    "To simplify the task, we are going to use only two features, the **sepal width** and the **petal width**. Furthermore, we will reduce our dataset to only contain **setosa** and **versicolor** species, i.e., we will drop the **virginica** species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90859f5a-3706-4dc0-8631-131e08e7884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the iris dataset\n",
    "iris = pd.read_csv(f\"{DATA_PATH}/iris.csv\")\n",
    "# Keep only sepal width, petal width, and species\n",
    "iris.drop(columns=[\"sepal length (cm)\", \"petal length (cm)\"], inplace=True)\n",
    "# Drop all observations of the species virginica\n",
    "iris = iris.loc[iris[\"species\"] != \"virginica\"]\n",
    "# Shuffle the dataset\n",
    "iris = iris.sample(frac=1)\n",
    "iris # Display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534fd5f-9e3f-4512-a06e-6c6caf1e3e2e",
   "metadata": {},
   "source": [
    "___\n",
    "#### ü§î <font color=green>**Question 1**</font>\n",
    "Notice how, in the above code, we use `iris.sample(frac=1)` to shuffle our data. Why would we prefer the data to be randomly ordered? How would things turn out if we left out the reshuffling?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8c48c-130f-46f2-b46a-a00883a0ad76",
   "metadata": {},
   "source": [
    "### From `pandas` to `numpy`\n",
    "\n",
    "While `pandas` is very intuitive when it comes to handling tabular data, e.g., for data pre-processing and visualization, `numpy` really shines when it comes numerical computing and it is somewhat closer to the mathematical formulation. Because of this, we will transform our data to `numpy` arrays which are useful to represent vectors and matrices.\n",
    "\n",
    "We have a dataset of size $N=100$ with with two features $\\mathbf{x}_1$ and $\\mathbf{x}_2$ (in our case petal length and sepal length), and a target $\\mathbf{y}$ (in our case species) which we are trying to predict based on the information from the features. Both features $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are numerical variables, so they are ready for mathematical and statistical calculations. However, we cannot say the same about out target variable species which contains string values. Machine learning completely happens in the language of numbers mathematics, so we need to translate the information in species into numbers. One possible way to do so is to assign the value $-1$ to every **setosa** observation and the value $+1$ to every **versicolor** observation. This is somewhat arbitrary, as there are many other ways to encode this variable in numbers. They would all result in a learning process that is equally good. E.g., an alternative example would be using target labels $0$ and $+1$. Both types of \"translations\" ($-1$ and 1, and $0$ and $1$) are very common. For now, we use the first version since this allows us naturally to talk about \"negatives\" (setosa) and \"positives\" (versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3314d-da49-4215-ba54-9ef520d41844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the matrix of features\n",
    "X = np.array(iris[[\"sepal width (cm)\", \"petal width (cm)\"]])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85861db4-bc20-45bf-9823-72f41626b797",
   "metadata": {},
   "source": [
    "We mentioned above how $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are our two features. We can also use a matrix $\\mathbf{X} = [\\mathbf{x}_1 \\, \\mathbf{x}_2]$ to represent our features. This matrix is a simple $100 \\times 2$ matrix, i.e., the first colum is the sepal width and the second one is the petal width. Every row represents a different observation. The matrix with the feature value is often called a **design matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0446e8-5a08-49f4-9fe3-4b36fddfc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector of labels / targets\n",
    "y = np.where(iris[\"species\"] == \"versicolor\", 1, -1)\n",
    "\n",
    "# If you forgot what np.where() is good for, go back to notebook 01d_numpy and study the section on filtering\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a81f9c-fca0-4537-b729-fbbaf4d46afe",
   "metadata": {},
   "source": [
    "For the labels, we dont need a matrix, a column vector of length $N=100$ will do just fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b8628-87cb-4de7-8d7c-0865e87db469",
   "metadata": {},
   "source": [
    "### The learning engine\n",
    "\n",
    "Now we implement the learning engine (and *you* will find out which lines of code below are the core part of that engine!). We could first discuss the theory of how and why this learning engine works. But we intentionally choose an \"experience-first\" approach and discuss the theory later. So all the guidance you have for making sense of the below code is our Excel implementation of the learning machine from the first lecture. (You can find a clean Excel solution on Canvas under Files > Data. It may also help to look again at the instructions for the Excel task in the slides for the first class.)\n",
    "\n",
    "Still, let's introduce a small piece of theory. In the first class with Excel, we called the items that are learned \"weights\". We now refine that language a bit, so it is in line with neural network terminology. If you go back to our Excel solution, you see that an important element in the construction of a learning machine was a score. We calculated this as\n",
    "$$\n",
    "score = w_0 + w_1 \\; petal\\_width + w_2 \\; sepal\\_width\n",
    "$$\n",
    "\n",
    "In line with neural-network terminology, we now denote $w_0$ as $b$ and call it a \"bias\". Some people also refer to it of a constant. In any case, it's still a parameter that is going to be learned and that plays a special role. More on this below. The advantage of the change in naming is also that the code becomes a bit easier to understand. The formula for the score becomes then\n",
    "\n",
    "$$\n",
    "score = b + w_1 \\; petal\\_width + w_2 \\; sepal\\_width\n",
    "$$\n",
    "\n",
    "\n",
    "As for the two weights $w_1$ and $w_2$, we collect them in a single vector $w$, as you will see in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e801a-ec8f-4ce0-8e62-b14f299ec135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for the learning process\n",
    "\n",
    "eta = 0.01 # The learning rate, this is an example of what is called a HYPERPARAMETER\n",
    "b = 0 # The bias\n",
    "w = np.zeros(X.shape[1]) # The weights (one for each feature)\n",
    "\n",
    "# Initialize lists for bookkeeping\n",
    "bias_list = []\n",
    "weights_list = []\n",
    "\n",
    "\n",
    "# Iterate over each iris case\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "\n",
    "    # Extract the ith row of the features matrix\n",
    "    x_i = X[i, :]\n",
    "    # Extract the ith row of the label vector \n",
    "    # (because it is a vector and not a matrix, there is no second dimension!)\n",
    "    y_i = y[i]\n",
    "    \n",
    "    # Compute the score\n",
    "    score_i = b + w[0] * x_i[0] + w[1] * x_i[1]\n",
    "    # We could have used matrix multiplication for this, but in the interest of transparancy, we did not do so\n",
    "    \n",
    "    # Make a prediction based on the score\n",
    "    # -1 (setosa) if score is negative (or zero), +1 (versicolor) if score is positive\n",
    "    pred_i = 1 if score_i >= 0 else -1  # Note this elegant shortcut of a conditional statement!\n",
    "    \n",
    "    # Bookkeeping of current weights and biases before we update them\n",
    "    bias_list.append(b)\n",
    "    weights_list.append(w.copy())  # See further below for an explanation on the use of w.copy. It's a bit tricky.\n",
    "    \n",
    "    # Update the weights and bias\n",
    "    b += eta * (y_i - pred_i)\n",
    "    w += eta * (y_i - pred_i) * x_i\n",
    "    \n",
    "# Get the output\n",
    "print(f\"The resulting bias is {b}, the resulting weights are {w}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a54de1-8b85-4844-9ab3-d0d691b4a340",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 2**</font>\n",
    "\n",
    "1. The above piece of code contains a loop. So it's a bit hard to just \"click around\" to find out what this code does. Obviously, to better understand, it would be nice to see what the code does for the first value of `i`, the second value of `i` etc., until you have a good grasp of what is going on. Find a procedure that allows you to do so. You can be creative, everything is allowed, as long as you do not destroy the code. **Hint.** if you mark several lines of code and then use the keyboard shortcut `ctrl + /` (do not type the plus, it means press `ctrl` and press `/`), you can \"out-comment\" these lines and they will not be executed if you run a code cell with `shift + enter`.\n",
    "2. Does the above loop somewhow also manifest in our Excel solution from the first class? How?\n",
    "3. In the above code, what lines would you say are the core of our learning engine?\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0fd29-f78b-4e7c-a3ff-1ba476ac9b19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ‚ö†Ô∏è Lists and copies (you can skip this in a first reading)\n",
    "Do you see how we used `w.copy()` instead of just appending `w` to our `weights_list`? This is because of a fairly complicated concept that really relates more to computer science than data science. In essence, Python lists are truly **arrays of pointers**. This surely doesn't mean much to you now, but try running the following cells to see why using a copy makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194349d-fa9a-462f-826f-21d9ffd6e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlist = [] # Create an empty list, e.g., this would be weights_list above\n",
    "wl = np.array([0, 0, 0]) # Create an array of weights (e.g., w above)\n",
    "wlist.append(wl) # Append the weights to our main list\n",
    "wlist # Display the value of wlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a725283-6cfc-4758-9bfa-4f6fa7e97dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl[1] += 1 # Change the value of the middle element\n",
    "wlist # Display the value of wlist again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b875d9d-4e72-49e6-b29f-13bf040997d1",
   "metadata": {},
   "source": [
    "But wait, didn't we actually store the array `[0, 0, 0]` and not `[0, 1, 0]`!? Let's do it again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ef96c-c24d-413a-9a22-e4136fe385d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlist.append(wl) # Append the new weights to our main list\n",
    "wl[1] += 1 # Change the value of the middle element again\n",
    "wlist # Display the value of wlist again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d9a1d-6844-4f47-8c6f-259d1c67ffe0",
   "metadata": {},
   "source": [
    "Uh oh... that's not good. We are *retroactively* affecting the weights we have already stored in our main list! What if we use a copy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbb27c-3ec1-4723-93bb-826c283e3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlist.append(wl.copy()) # Append the weights COPY to our main list\n",
    "wl[1] += 1 # Change the value of the middle element again\n",
    "wlist # Display the value of wlist again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40cbff-9d29-4ff6-bdc2-814dfad48bde",
   "metadata": {},
   "source": [
    "The third item of our list was not impacted by the increment. This is because it's not the weight array anymore but a copy thereof. Make sure you understand what happened in this small section, this is a dangerous pitfall of Python that surely happens to every programmer at least once (and probably many more times...).\n",
    "\n",
    "Intuitively, you can think that Python keeps in mind that the elements of `wlist` are the arrays `wl` and it assumes that we want to keep a link between the two. So `wlist` should remember that it contains `wl` and that if `wl` gets changed, so should `wlist`. In fact, Python does not keep `wlist` in memory as such. All it has in memory is the instruction that `wList` is a list that contains `wl` and to fill in whatever `wl` happens to be at the moment. If you want to break the link between `wlist` and `wl` (which, as a data scientist, you want most of the time) then you use `.copy()`. Does it now make sense?\n",
    "\n",
    "But wait, if this is so with lists, why didn't we treat the bias `w` in the same way? Why use `weights_list.append(w.copy())` but **NOT** `bias_list.append(b.copy())`? What we just explained only applies if the object that is appended is already a type of list, e.g. a numpy array. If it is a primitive object like `b` (a simple variable that stores a single number), then there is no need for `.copy()`. In fact, if you try, you get an error. That's nice, so you know you don't have to use it!\n",
    "\n",
    "\n",
    "**Main takeaway**: Be careful when storing results from lists, arrays, dictionaries, etc. in other containers (typically lists or dictionaries). Changing the item ex-post will also change the value stored in your *larger* container. If you want to avoid this (which you want most of the time) make sure to use a copy!\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ef1f1-6a12-46bf-9ff7-f6b675aa19dc",
   "metadata": {},
   "source": [
    "#### Calculating learning diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85b1c6-b1a3-47e5-9b99-7398f68cb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we evaluate how the learning machine performs in its task...\n",
    "# We do so using certain standard \"performance indicators\" that serve as learning diagnostics\n",
    "\n",
    "# Create some empty lists as containers that are going to be filled\n",
    "misclassifications = []\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "# Iterate over the learning steps of the perceptron algorithm\n",
    "for i in range(len(bias_list)):\n",
    "    # Compute score over FULL dataset (notice the matrix multiplication!)\n",
    "    score = bias_list[i] + X @ weights_list[i].T\n",
    "    \n",
    "    # Compute the prediction over the full dataset\n",
    "    pred = np.where(score >= 0, 1, -1)\n",
    "    \n",
    "    # Compute missclassification, false positives, false negatives\n",
    "    error = y - pred\n",
    "    misclassifications.append(sum(error != 0))\n",
    "    false_positives.append(100 * sum(error < 0) / sum(y == 1))\n",
    "    false_negatives.append(100 * sum(error > 0) / sum(y == -1))\n",
    "\n",
    "print(misclassifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6acb0-7f07-4a24-928c-f407279ef8b0",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 3**</font>\n",
    "\n",
    "1. Explain in your own words what false positives, false negatives and misclassifications are.\n",
    "2. Use your tricks identified in Question 3 to inspect what is going on in the loop. How does `weights_list[i]` look for $i = 5$? What is its precise dimension? What is the dimension of the multiplication `X @ weights_list[i].T`? What does the `.T` mean? Would you really need it?\n",
    "3. What is the dimension of `error` for any $i\\in \\lbrace 0, 1, \\ldots, 99 \\rbrace$?\n",
    "4. Does the above loop somewhow also manifest in our Excel solution from the first class? How?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133451c3-0df4-47ed-9639-e5412868cee7",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 4**</font>\n",
    "Create a scatter plot of the data such that:\n",
    "+ the sepal length is displayed on the x-axis\n",
    "+ the petal length is displayed on the y-axis\n",
    "+ setosa data points are colored in blue and versicolor data points are colored in green\n",
    "+ there is a legend showing which color belongs to which iris species\n",
    "+ the x- and y-axis are labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36762038-7d46-4f60-8aaa-0ba04cde259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5128711-5de8-43c8-985c-6bf8a20db05e",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0ad49-083f-495c-a2e2-bb984275e29a",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 5**</font>\n",
    "In fact, the perceptron algorithm as used here separates the feature space linearly, i.e., it *draws a line* in the above plot. Using the final weights and bias obtained by our algorithm, can you characterize this line in a mathematical equation? *Hint:* Express the equation in the form $x_2 = a + m \\cdot x_1 $. Do not use any coding but use paper and pencil (maybe a digital version of those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634dc441-cbbe-4b9c-9907-a4fed47a22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the final optimal values obtained by the perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adceff2d-a82d-437e-8740-ec749e9b57ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16c247-790a-485d-8d28-297dec784c75",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 6**</font>\n",
    "\n",
    "1. Using the line equation you have determined in the previous task, augment the plot you created in Question 4 by drawing the perceptron classification line.\n",
    "\n",
    "    *Hint:* Think of your plot as an x-y-plane. You want to define an array of x values (horizontal variable) going from the minimum to maximum of the sepal width. Then, using the equation derived above, you want to map a y value (vertical variable) for each of those points, e.g.,\n",
    "\n",
    "    ```python\n",
    "    # Create a linearly space vector of x's from the minimum to the maximum sepal width\n",
    "    x_values = np.linspace(X[:, 0].min(), X[:, 0].max())\n",
    "    # Create the linear relationship derived above (replace a and m by the values you found!)\n",
    "    y_values = a + m * x_values\n",
    "    ```\n",
    "\n",
    "    With `x_values` and `y_values` we refer generically to a variable on the horizontal and on the vertical axis in a 2D plane, respectively. (I.e. we do not mean features and target here.)\n",
    "\n",
    "\n",
    "2. Why actually do we benefit from including the bias $b$ in the perceptron model? Try to answer this questions based on your calculation in Question 5 and the plot you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9298c-294f-4022-a69d-3ad09ae661c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078cc2f-6f6c-46c6-af79-0f36f6aaadb0",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e7d85-10f4-44f3-b0b3-a7ea145d70b7",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 7**</font>\n",
    "\n",
    "Create a visualization of the learning process. This plot should contain the following:\n",
    "+ the iteration numbers on the horizontal axis\n",
    "+ a dashed line with the number of misclassifications in blue\n",
    "+ a dash-dotted line with the false positive rates in green\n",
    "+ a dotted line with the false negative rates in orange\n",
    "+ add a grid\n",
    "+ don't forget to label your axes and add a legend!\n",
    "\n",
    "Discuss and interpret the results of this plot with your classmates. Are you surprised by what you see? Using your common sense, do you like the result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028bb12-046b-41a2-8b72-53d060a44958",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9537e-e405-43c6-8256-721bd0bf936b",
   "metadata": {},
   "source": [
    "___\n",
    "#### ‚û°Ô∏è ‚úèÔ∏è<font color=green>**Question 8**</font>\n",
    "Where do you see the biggest advantage of using Python over Excel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0adff-509d-466a-8ca3-e685966da020",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## The mathematics of perceptron learning\n",
    "\n",
    "We will go through the mathematics of perceptron learning together based on the slides for this class. However, for those who like to dig a little deeper, we have written down the math in a somewhat more general form below.\n",
    "\n",
    "The perceptron model actually represents a special case of a neural network, generally considered the simplest one. Understanding how the perceptron works will not be enough to understand all neural networks, but it is a necessary first step, so let's dive into it!\n",
    "\n",
    "We have a dataset consisting of $N$ pairs of features and labels: $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\dots, (\\mathbf{x}^{(N)}, y^{(N)})\\}$, where $\\mathbf{x}^{(i)} \\in \\mathbb{R}^p$ is vector with dimension $p$ ($p$ is equal to the number of features, i.e., in our example $p=2$), and $y^{(i)} \\in \\{-1, +1\\}$. (Note: when we do math, we prefer start counting at 1, not at 0; we stick to that convention in the course.)\n",
    "\n",
    "Using the *activation function*: $\\begin{aligned}f({z}) = \\begin{cases}1 \\, &\\text{if } {z} > 0 \\\\ -1 &\\text{otherwise} \\end{cases} \\end{aligned}$, the perceptron's goal is to find a scalar bias $b$ and a vector $\\mathbf{w} \\in \\mathbb{R}^p$, such that the objective $\\sum_{i=1}^N |y^{(i)} - f(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)|$ is minimized (the two vertical bars indicate the absolute value of the difference).\n",
    "\n",
    "To minimize this quantity, the algorithm takes the following steps:\n",
    "1. Initialize the bias and weights arbitrarily, e.g., $b = 0$ and $\\mathbf{w} = [w_1 \\, w_2]^\\top = [0 \\, 0]^\\top$. Define a learning rate $\\eta$.\n",
    "2. For each example $i$ in the dataset $D$, do the following:\n",
    "    1. Compute the output given the current weights and bias:  \n",
    "    $\\begin{align}\\hat{y}^{(i)} &= f(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\\\ &= f(w_1 \\cdot x_1^{(i)} + w_2 \\cdot x_2^{(i)} + b)\\end{align}$\n",
    "    2. Update the weights and bias ($\\eta$ is the learning rate):  \n",
    "    $b \\leftarrow b + \\eta \\cdot \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$  \n",
    "    $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\cdot \\mathbf{x}^{(i)}$\n",
    "3. We may want to repeat the second step until the prediction error is *good enough*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
