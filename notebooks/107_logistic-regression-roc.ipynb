{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6e56d5-7d00-4866-a7cc-d3e1e675040b",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/JLDC/Data-Science-Fundamentals/blob/master/notebooks/107_logistic-regression-roc.ipynb\">\n",
    "    <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Open this notebook in Google Colab\n",
    "</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce1e86-2cc5-4fc0-b3f3-83588bcbc636",
   "metadata": {},
   "source": [
    "# Logistic Regression for Bank Loan Defaults\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07eca79-40eb-4dc0-9c41-3a37ebbf69ed",
   "metadata": {},
   "source": [
    "The *logistic* regression is very similar to *linear* regression, but it is used for **classification** problems, whereas linear regression is used for **regression** problems. Recall that a **regression** problem is one where the target variable is a numerical variable where the numerical values have real meaning. Examples include crop yield, the price of a stock, temperature, sales, blood pressure... A **classification** problem, on the other hand, consists of predicting a *categorical* target, such as predicting whether a bank loan will default, whether an iris belongs to the *setosa* or *versicolor* species, or whether the price of a stock goes *up* or *down*.\n",
    "\n",
    "___\n",
    "#### 🤔 Pause and ponder\n",
    "Did you pay attention above? Predicting the price of a stock is a *regression* problem but predicting whether it will go up or down is a *classification* problem. Does it make sense? Is it not the same problem? Can you think of other prediction tasks where we could take both a *regression* and a *classification* perspective?\n",
    "___\n",
    "\n",
    "Both linear and logistic regressions are the prototypical *plain vanilla* models for regression and classification problems, respectively. They can be seen as the natural starting point in nearly all applications (except when you have *non-tabular data* like in computer vision or natural language processing). \n",
    "\n",
    "Remember the notebook on MSE visualization, with cancer types from the WDBC data. This was in fact also a classification problem. However, when deriving weights by minimizing the MSE using gradient descent, we have treated it as a regression problem! Actually, the weights we derived by gradient descent are the exact same weights of an OLS regression! \n",
    "\n",
    "While this is fine to a certain extent, in practice we usually prefer a different approach to classification problems where the models predict a number between 0 and 1, such that it can be interpreted as a *probability* or *share*. By contrast our score ($w_1 \\cdot x_1 + w_2 \\cdot x_2$) could, in theory, take on any values. A logistic regression provides a simple way to convert a score into a number between 0 and 1 that we can thus interpret as a probability. \n",
    "\n",
    "In fact, there is a neat function called the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which takes as input any real number and outputs a number between 0 and 1, looking at the plot below, can you see how this function can be useful to map our unbounded score to a probability range?\n",
    "\n",
    "The sigmoid function is given as \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = 1 - \\sigma(-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168945c-85f7-4fa3-a371-79d4f792655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Plotting\n",
    "import numpy as np # Numerical computations\n",
    "import pandas as pd # Dataframes\n",
    "\n",
    "# Define the path where the data is stored\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/JLDC/Data-Science-Fundamentals/master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee599ee2-329a-4c5b-9d06-0c09fa5eee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create canvas\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "# x-axis, 100 linearly spaced points between -7 and +7\n",
    "xs = np.linspace(-7, 7, num=100)\n",
    "# y-axis, sigmoid function\n",
    "ys = 1 / (1 + np.exp(-xs))\n",
    "# Draw sigmoid function\n",
    "ax.plot(xs, ys)\n",
    "# Add ticks, grid, title\n",
    "ax.set_yticks(np.linspace(0, 1, num=11))\n",
    "ax.grid(True)\n",
    "ax.set_title(\"Sigmoid function\")\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$\\sigma(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396720a-95a1-496f-96ca-5fde006e75e3",
   "metadata": {},
   "source": [
    "___\n",
    "## Data pre-processing\n",
    "We will work with data on bank loans to illustrate a use-case of logistic regression. If you feel like, however, it is a good exercise to replicate parts of this notebook using the WDBC dataset instead, as it also features a categorical target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d617a9-903d-4c7c-9b96-3e1741a00f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the bank loan dataset\n",
    "loans = pd.read_csv(f\"{DATA_PATH}/data/loan_defaults.csv\")\n",
    "loans # Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ff573-1d0e-4aae-8fe9-94906be03a3a",
   "metadata": {},
   "source": [
    "As we can see from the excerpt above, we have quite a lot of data (195k observations) and there are a lot of different variables (columns) in our dataset. Normally, you aren't just *given* some dataset, but you obtained it from a specific source and you can find some information on what each variable represents or how it is measured. This is not the case here, so you have to rely on the column names...\n",
    "\n",
    "In any case, our target variable is the `Loan_Status`, and a `1` indicates a default, i.e., that the loan was not paid back. As you can already see, without this particular information, it would be hard to understand only from the data whether `0` or `1` means a default.\n",
    "\n",
    "____\n",
    "#### 🤔 Pause and ponder\n",
    "When you have some dataset and you don't know what the value means exactly (e.g., if we hadn't given you this information on `Loan_Status`), is it still possible to make an educated guess as to which number means what? How would you proceed?\n",
    "___\n",
    "\n",
    "First, let's check the value of `Loan_Status`, if there are any missing values, and if it is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec511d-7a25-46fd-ae38-351e175c4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of values for the prediction label\n",
    "loans[\"Loan_Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df497b41-b38a-4e0d-b69a-ea60816200c8",
   "metadata": {},
   "source": [
    "There are approximately half as many defaults as there are repaid loans, this is expected. The nice thing is that there are no missing values for our label. What about the other columns, do they have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b2421-6eb3-4e1f-a7af-f4256d06e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the percentage of missing values in each column\n",
    "loans.isnull().sum() / loans.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7b264-01d9-470f-bbca-89013f6cb579",
   "metadata": {},
   "source": [
    "`Months_since_last_delinquent` has a whopping 54.71% of missing values, and `Current_Loan_Amount` also has quite a few, with 18.02% ! This raises an issue we have largely ignored until now: how do we deal with missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a560d0-bcfb-4dc4-8d68-a4d2099701f0",
   "metadata": {},
   "source": [
    "___\n",
    "### Handling missing data\n",
    "Unfortunately, there is no *best* way to handle missing data. As with many aspects of data science, it is more of an art than a science. Here are some possible approaches and what drawback they might entail:\n",
    "\n",
    "#### 1. Removing columns with missing values\n",
    "This is what we will do for `Months_since_last_delinquent`, more than half of the data is missing so it really is an extreme case. However, when choosing this approach, we might be dropping important variables that might help us for our prediction. This is something we would like to check in general, e.g., if the 45% of non-missing data in this column was able to perfectly predict the loan defaults, we would obviously still prefer keeping it and we would deal with the missing values in a different way.\n",
    "\n",
    "#### 2. Removing rows with missing values\n",
    "What if instead of dropping the entire column, we simply dropped every row that has a missing value in any of the column? This also works. However, this can reduce our sample size significantly. For instance, we do not want to drop 55% of our dataset because a single variable is missing, that might impact the analysis too much. Furthermore, what if there is a *reason* why the data is missing? For instance, there might be a *structure* to the missing data and by ignoring the missing data, we might be biasing our model strongly.\n",
    "\n",
    "For instance, consider that in some geographical area, the data is missing because it was too hard to gather. If we drop missing data, our model will probably perform badly when it comes to predicting observations in that particular area.\n",
    "\n",
    "#### 3. Imputing missing values\n",
    "A third method of dealing with missing values is to *impute* them, i.e., replace them with some other value. For example, we might want to replace the `Current_Loan_Amount` values with the mean of the non-missing values. This would allow us to keep our full dataset instead of discarding 18% of it if we were to remove the rows as mentioned above in 2.\n",
    "\n",
    "The difficulty of this approach is to choose an imputation strategy. Using the mean or the mode of the non-missing values is the simplest way to do it, but it might not be the best. In fact, if there are a lot of missing observations, it might make things worse.\n",
    "\n",
    "#### 4. Recoding\n",
    "In our case, a missing value for `Months_since_last_delinquent` may mean that the person has never been delinquent on a payment so far. We may create a new variable `ever_delinquent` and set it for all missings to 0 and for non-missings to 1. We could also set the missings to a very high value. Literally speaking, the number of months since last delinquency is infinite (or the age of the person) for those who have never been delinquent. Whether this is a good idea depends also whether you can show to improve the predictive performance of the model with this procedure.\n",
    "\n",
    "___\n",
    "\n",
    "As you can take away from this discussion, handling missing data is complex and full of somehow a little arbitrary decisions. It is important to double check with our data that our chosen strategy isn't impairing our model, and, most importantly, we must be able to justify our choices.\n",
    "\n",
    "In this case, learning to deal with missing data is not the main objective of this notebook. So we will take the easy way out: drop `Months_since_last_delinquent` and drop all rows that have missing values anywhere else. But be mindful that we could surely obtain a better model by spending more time and effort on our missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c607f8b-6a0c-4050-8c04-fafc5e4ae90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Months_since_last_delinquent column\n",
    "loans.drop(columns=[\"Months_since_last_delinquent\"], inplace=True)\n",
    "# Drop the rows with missing values\n",
    "loans.dropna(inplace=True)\n",
    "loans # Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6454a-7408-43b7-b9fa-ab2c44626d97",
   "metadata": {},
   "source": [
    "After this cleaning step, we are left with 152k observations.\n",
    "\n",
    "As you can see, many of the variables have string values (text). In a statistical model, we need numerical values for all\n",
    "features (and for the target), so we will need to process these values in a way.\n",
    "\n",
    "#### Dummy-encoding and One-hot-encoding\n",
    "The most common way to encode non-numerical variables is the **dummy-encoding** or **one-hot-encoding**. While there is a difference in the encodings, the names are often used interchangeably. As a matter of fact, dummy-encoding is a special case of one-hot-encoding, when there are only two variables. So, one-hot-encoding is just the more general encoding.\n",
    "\n",
    "Take the variable `Term` in our dataframe, it can take values `Short Term` or `Long Term`. This is perfect for dummy-encoding, e.g., we can replace `Short Term` with a `1` and `Long Term` with a `0`. This way, we have replaced the string variable by a single number.\n",
    "\n",
    "Take `Home_Ownership` on the other hand. It can take values `Home Mortgage`, `Own Home`, `Rent`, `HaveMortgage` instead, so up to 4 different values, and we won't be able to dummy-encode it. Instead, we can use one-hot-encoding, the idea is to create 3 new variables, also called dummies, (always one less than the possible values, same for dummy-encoding!):  \n",
    "1. `Home_Mortgage`: takes value `1` if the `Home_Ownership` is equal to `Home Mortgage`, `0` otherwise.\n",
    "2. `Own_Home`: takes value `1` if the `Home_Ownership` is equal to `Own Home`, `0` otherwise.\n",
    "3. `Rent`: takes value `1` if the `Home_Ownership` is equal to `Rent`, `0` otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b34d09-cb62-48cf-884e-4e1b831692b7",
   "metadata": {},
   "source": [
    "___\n",
    "#### ➡️ ✏️<font color=green>**Question 1**</font>\n",
    "\n",
    "Why do we need one less dummy than the number of possible values a variable can take? Are we not forgetting about `HaveMortgage` or `Long Term`? Can we just get away with discarding information like that?\n",
    "___\n",
    "\n",
    "\n",
    "`pandas` provides a useful function to encode text data, namely the `get_dummies()` function. Let's go ahead and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e62e20-1ea0-42a9-b21e-2ab2a191340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's have a look at how the function works on a single column\n",
    "# Notice the use of `drop_first=True`, try to see what happens if you set it to false\n",
    "pd.get_dummies(loans[\"Home_Ownership\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915576bc-edc6-424d-867c-bcc277f8d5e4",
   "metadata": {},
   "source": [
    "Unfortunately, the above only returns the encoding for a single column, so we would have to reconstruct our dataframe by replacing every string column with its encoding. This seems cumbersome. Luckily, `pandas` is very flexible, and we can make `get_dummies` work on the full dataframe, transforming only the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6bcf4-c89a-420c-bc6b-b1afe26cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of columns which have an 'object' type (string)\n",
    "str_cols = [col for col in loans.columns if loans[col].dtype == \"O\"]\n",
    "\n",
    "# For understanding what the code does check this\n",
    "loans[\"Home_Ownership\"].dtype\n",
    "# A type ‘O’ just stands for “object” which in Pandas’ world is a string.\n",
    "\n",
    "print(str_cols)\n",
    "\n",
    "# Encode these columns\n",
    "loans = pd.get_dummies(loans, columns=str_cols, drop_first=True)\n",
    "loans # Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ffff-b891-4064-869e-062178d6fcb9",
   "metadata": {},
   "source": [
    "___\n",
    "## Logistic regression on the entire dataset\n",
    "Let's start with a first logistic regression estimation on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fc236-380c-4ada-bcbd-2ba20ad7fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e393d-1847-49d0-a39e-9abbcd7e59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y variables\n",
    "X = loans.drop(columns=[\"Loan_Status\"])\n",
    "y = loans[\"Loan_Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cca958-8890-45e3-8369-b2df3a507e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "model = LogisticRegression()\n",
    "# Fit it to the data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35925aa8-778c-42b9-89ef-57ead4eed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted label and probability of default\n",
    "pred = model.predict(X)\n",
    "pred_prob = model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772ee17-2418-4766-a7cb-5ea27b0b866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error metrics\n",
    "N = loans.shape[0] # Number of observations\n",
    "# Misclassification percentage\n",
    "misclass = np.mean(y != pred)\n",
    "# False positive rate\n",
    "false_pos = np.mean(pred[np.where(y == 0)])\n",
    "# False negative percentage\n",
    "false_neg = 1 - np.mean(pred[np.where(y == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf26bd-132b-4d0d-b045-ac954fbf068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print error metrics\n",
    "print(f\"The missclassification rate is {100 * misclass:.2f}%\")\n",
    "print(f\"The false positive rate is {100 * false_pos:.2f}%\")\n",
    "print(f\"The false negative rate is {100 * false_neg:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea720015-17e1-49b1-ad99-38a60e8b11ce",
   "metadata": {},
   "source": [
    "___\n",
    "#### ➡️ ✏️<font color=green>**Question 2**</font>\n",
    "Can we rely on these values? Why is the false negative rate so much higher than the false positive rate?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169c5b9-ab8a-4b6a-bc13-c5e5587e86c9",
   "metadata": {},
   "source": [
    "## Logistic regression with cross-validation\n",
    "Remember the discussion we had about training, testing, and cross-validation in the previous notebooks. Above, we have estimated the model on the entire sample and we have also use the same sample to evaluate the metrics. We know that this is a capital sin of data science, so let's do better!\n",
    "\n",
    "We will use cross-validation to evaluate our results. In the last notebooks, we emphasized that cross-validation helps us select a model, but here, we will not do any model selection. Instead, we will use cross-validation to evaluate the performance of our model out-of-sample. Now you might begin to realize why this differentiation between testing and validating is not always clear. But as we said, this is not extremely important. **Just make sure you never judge a model's performance based on the data you used to fit it and you will be fine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea22f4f-bb5d-441a-8715-7d2b7bcdd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate # CV function\n",
    "from sklearn.metrics import make_scorer # To make our custom scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de22b0-9020-4b4e-b901-0f1bcc01ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom scorers for false positive rate and false negative rates\n",
    "fpos_rate = lambda y_true, y_pred: np.mean(y_pred[np.where(y_true == 0)])\n",
    "fneg_rate = lambda y_true, y_pred: 1 - np.mean(y_pred[np.where(y_true == 1)])\n",
    "fpos_scorer = make_scorer(fpos_rate, greater_is_better=False)\n",
    "fneg_scorer = make_scorer(fneg_rate, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2f815-1220-4f78-a6b0-8f8b0f8d42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 10-fold cross-validation, return accuracy, FPR, FNR\n",
    "nfolds = 10\n",
    "cv_results = cross_validate(model, X, y, cv=nfolds,\n",
    "    scoring={\"accuracy\": \"accuracy\", \"fpr\": fpos_scorer, \"fnr\": fneg_scorer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b01ec-057d-473d-ae0a-5d77339d696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics for the cross-validation\n",
    "misclass_cv = 1 - cv_results[\"test_accuracy\"]\n",
    "false_pos_cv = -cv_results[\"test_fpr\"]\n",
    "false_neg_cv = -cv_results[\"test_fnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85502f-8137-4f93-950d-0cc442fe7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty helpers to compute mean and standard error thereof\n",
    "mean_and_se = lambda x, n: (np.mean(100 * x), np.std(100 * x) / np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348de71-3124-45c6-88b2-1e64e3585728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means and standard errors\n",
    "misclass_mean, misclass_se = mean_and_se(misclass_cv, nfolds)\n",
    "false_pos_mean, false_pos_se = mean_and_se(false_pos_cv, nfolds)\n",
    "false_neg_mean, false_neg_se = mean_and_se(false_neg_cv, nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acea10-b86e-411d-b996-88a460343ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print error metrics\n",
    "print(f\"The mean missclassification rate is {misclass_mean:.2f}% (± {misclass_se:.2f}%)\")\n",
    "print(f\"The mean false positive rate is {false_pos_mean:.2f}% (± {false_pos_se:.2f}%)\")\n",
    "print(f\"The mean false negative rate is {false_neg_mean:.2f}% (± {false_neg_se:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e83e3-cef1-4921-8db5-737cccedbdbd",
   "metadata": {},
   "source": [
    "___\n",
    "#### ➡️ ✏️<font color=green>**Question 3**</font>\n",
    "What do you conclude when comparing those values to the one obtained on the full dataset? Discuss with your classmates.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd492fc3-0c0f-4a5e-8023-ef5cc8153d0e",
   "metadata": {},
   "source": [
    "## The receiver operating characteristic (ROC) curve\n",
    "The [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve is a diagnostic visualization of a binary classifier's performance and how this performance changes as the discrimination threshold is varied.\n",
    "\n",
    "First, what is the **discrimination threshold**? We have mentioned above, that the output of the logistic regression is a value between 0 and 1 (see `model.pred_proba(X)`). But in the end, we are not using the predicted probabilities as the labels, instead we are classifying the labels based on the probabilities. E.g., if our prediction is at least 0.5, and we then classify the label as a 1, and, if our predicted probability is below 0.5, we then classify as a 0, then the value 0.5 our **discrimination threshold** (also called critical threshold).\n",
    "\n",
    "Of course, 0.5 seems like an intuitive threshold since it is *in the middle* between 0 and 1. But depending on the use case, it might be better to adjust this threshold. In fact, the ROC curve will help us select the optimal threshold for our goal.\n",
    "\n",
    "To learn about the ROC curve and how we can use it to tune our model, we will again work with a training and testing set, as we did in the OLS notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbba08-455a-4cab-814c-c1411d64f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Data splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4c15b-39b7-4778-aec2-af30563d4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3b8b2-fb7b-4a8c-b157-85a34bb6a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a logistic regression model\n",
    "model_t = LogisticRegression()\n",
    "# Fit the model on the traning data\n",
    "model_t.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f594ba-aa2f-4cd8-902e-fbbc1dc13918",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1280px-Roc_curve.svg.png\" style=\"width:300px\"></center>\n",
    "\n",
    "The above shows the ROC curve plot. It is a plot where the x-axis represents the false positive rate, and the y-axis represents the true positive rate. `sklearn` provides a function to directly plot the ROC curve from an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cd0bb-2d54-471e-9453-a0baaf9b842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eef854-b0ef-4e01-9083-fb99989ca829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fitted model to predict the train and test labels\n",
    "pred_prob_train = model_t.predict_proba(Xtrain)\n",
    "pred_train = model_t.predict(Xtrain)\n",
    "pred_prob_test = model_t.predict_proba(Xtest)\n",
    "pred_test = model_t.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb7d10-3588-45d4-a8fa-ed5fbf42ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ROCDisplay object from sklearn\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "RocCurveDisplay.from_estimator(model_t, Xtrain, ytrain, ax=ax, label=\"Train\")\n",
    "RocCurveDisplay.from_estimator(model_t, Xtest, ytest, ax=ax, label=\"Test\")\n",
    "# Add grid, title\n",
    "ax.grid(True)\n",
    "ax.set_title(\"Receiving Operator Characteristic Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3838c65-cdfc-4452-88b4-c608cc0cf676",
   "metadata": {},
   "source": [
    "___\n",
    "#### 🤔 Pause and ponder\n",
    "It's not always easy to get accustomed to the ROC curve, but try to think of it in an intuitive manner. For the logistic regression model we have set up, changing the discrimination threshold allows us to *slide along the curve*, e.g., if we choose a discrimination threshold of zero, everything will be classified as a 1 (because every probability will be above zero), and we will have a TPR of one and a FPR of one, i.e., we will be in the top right of the curve. On the other hand, if we set the discrimination threshold to one, we will be in the lower left corner, classifying everything as a 0.\n",
    "\n",
    "If you compare this to the above picture, it means that even with the best discrimination threshold possible, we can't reach the *perfect classifier* spot using this particular model. But on the flipside, we will always do better than the random classifier (unless we choose a discrimination threshold of zero of one).\n",
    "\n",
    "Did all of this make sense? It can be somewhat abstract at first, so don't worry if not, but make sure to ask if you have questions!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e1a8d-f0ee-4d1f-b1c6-84eb96f0e4dc",
   "metadata": {},
   "source": [
    "___\n",
    "#### ➡️ ✏️<font color=green>**Question 4**</font>\n",
    "Are the true positive rate (also called recall or sensitivity) and true negative rate (also called specificity) equally important? Can you think of problems where one is more important then the other?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4518d-0811-43bf-8971-50a23e8ab106",
   "metadata": {},
   "source": [
    "Of course, other metrics can also be helpful to evaluate the performance of our model. Let's do a confusion matrix. And then we calculate and plot the precision and recall based on the numbers in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83d591-303f-4283-83a5-01226bb4d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc51ba-1c99-4cc7-aa6e-47a661f5fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confustion matrix\n",
    "conf_mat_test = confusion_matrix(ytest, pred_test, normalize=\"all\")\n",
    "\n",
    "# Setting normalization to \"all\" means that we express all the numbers as a fraction of the overall total of all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cc7cc-ca69-4f40-875e-ac1153b71409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix in tabular format (using pandas)\n",
    "pd.DataFrame(100 * conf_mat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db99883-efd9-457c-919c-e1ae8295c0e0",
   "metadata": {},
   "source": [
    "To better understand the following calculations, note the following relationships. Denote true positives and true negatives by $TP$ and $TN$, respectively, and denote false positives and false negatives by $FP$ and $FN$, respectively. We then have\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$.\n",
    "\n",
    "Thus, precision gives the answer to the question: out of all cases predicted as positive, what share is actually positive? And recall gives us the answer to the question: out of all cases that *are* actually positive, what share is predicted (\"recalled\" by the model) as positive?\n",
    "\n",
    "___\n",
    "#### 🤔 Pause and ponder\n",
    "Can you see that there is a trad-off between precision and recall? I.e. higher value of one may mean a lower value of the other...\n",
    "\n",
    "___\n",
    "\n",
    "Now let's do the calculations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac99950-35fe-4441-8a3f-7f1ed750e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TPR, precision for different values of the discrimination threshold\n",
    "threshold = np.linspace(0, 1, num=100)\n",
    "# Initialize empty lists for TPR and precision (not very efficient, but easier)\n",
    "tpr = np.empty(threshold.shape)\n",
    "precision = np.empty(threshold.shape)\n",
    "# Iterate over thresholds\n",
    "for i in range(threshold.shape[0]):\n",
    "    # Make predictions according to threshold\n",
    "    ypred = pred_prob_test[:, 1] >= threshold[i]\n",
    "    # Compute (absolute values of) TP, FP, TN, FN using the confusion_matrix\n",
    "    (tn, fp), (fn, tp) = confusion_matrix(ytest, ypred)  # Note that we do not use the normalize argument here!\n",
    "    tpr[i] = tp / (tp + fn)\n",
    "    # For precision, we need to be careful with division by zero!!\n",
    "    if tp + fp > 0:\n",
    "        precision[i] = tp / (tp + fp)\n",
    "    else: # Add a NaN object\n",
    "        precision[i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61241a7d-bd07-40b8-9950-325e546da660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the TPR and precision as a function of the probability cutoff\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Plot FPR, TPR\n",
    "ax.plot(threshold, tpr, label=\"True positive rate\")\n",
    "ax.plot(threshold, precision, label=\"Precision\")\n",
    "# Add x-label, grid, legend, ticks\n",
    "ax.set_xlabel(\"Discrimination threshold\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "ax.set_xticks(np.linspace(0, 1, num=11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387ed57-92a5-40c0-acfb-33b02e215919",
   "metadata": {},
   "source": [
    "___\n",
    "#### ➡️ ✏️<font color=green>**Question 5**</font>\n",
    "\n",
    "1. Translate this plot into plain language, such that the bank's managers could understand it! \n",
    "2. What happens with the precision just after the discrimination threshold 0.7?\n",
    "3. Suppose we automate our credit decision procedures based on our logistic model. Does the choice of a particular critical probability threshold have an implication on the bottom line of our business? Why or why not? How or how not?\n",
    "4. How do you decide about the best combination of precision and recall?\n",
    "5. Would you want other metrics to make an informed decision for the bank? If so, what information would you additionally want to consider?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
